{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9c5eed-3731-4a0b-8ce1-6abcbc22ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import wikipedia\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import logging\n",
    "from scipy.stats import spearmanr\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "np.set_printoptions(precision=4, threshold=1000, linewidth=10000, suppress=True, floatmode='fixed')\n",
    "path_base = 'D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\\\'\n",
    "\n",
    "# Load wordnet\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.data import path # need to specify the location of the nltk data\n",
    "path.append(\"D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\Frames and Structured Data\\\\nltk_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6929d60-4c69-4dd5-b576-b0544a8806f1",
   "metadata": {},
   "source": [
    "### Functions for loading wiki articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "897582ed-a5e9-405f-96bb-b81982981e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if given wiki article can be loaded, needed to avoid crashes for loading non-existent articles\n",
    "def check_wiki_article(title, printing=False):\n",
    "    \"\"\" string -> bool\n",
    "    Returns True if the article corresponding to the inputted title can be loaded, False if not.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = wikipedia.page(title) # load the wiki article\n",
    "        if printing==True:\n",
    "            print('Loaded: '+article.title)\n",
    "            print(article.content[0:100]) # show preview of article\n",
    "        loaded = True\n",
    "    except:\n",
    "        loaded = False\n",
    "    \n",
    "    if loaded==False:\n",
    "        try:\n",
    "            alt_title = title+'s' # plural sometimes works\n",
    "            article = wikipedia.page(alt_title)\n",
    "            if printing==True:\n",
    "                print('Loaded: '+article.title)\n",
    "                print('Title used: '+alt_title)\n",
    "                print(article.content[0:100]) # print a preview\n",
    "            loaded = True\n",
    "        except:\n",
    "            loaded = False\n",
    "    \n",
    "    if loaded==False:\n",
    "        try:\n",
    "            alt_title = title+title[-1] # sometimes this works for some reason\n",
    "            article = wikipedia.page(alt_title)\n",
    "            if printing==True:\n",
    "                print('Loaded: '+article.title)\n",
    "                print('Title used: '+alt_title)\n",
    "                print(article.content[0:100]) # print a preview\n",
    "            loaded = True\n",
    "        except:\n",
    "            if printing==True: \n",
    "                print('not found')\n",
    "            loaded = False\n",
    "            \n",
    "    return(loaded)\n",
    "\n",
    "\n",
    "# Load plain text of single wiki article\n",
    "def load_wiki_article(article_title):\n",
    "    \"\"\" string -> list\n",
    "    Loads the wikipedia article corresponding to the given title, returning its content as a list of sentences.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = wikipedia.page(article_title) # load the wiki article\n",
    "        article_sentences = split_to_list(article)\n",
    "    except:\n",
    "        try:\n",
    "            alt_title = article_title+'s' # plural sometimes works\n",
    "            article = wikipedia.page(alt_title)\n",
    "            article_sentences = split_to_list(article)\n",
    "        except:\n",
    "            print(article_title+' not found')\n",
    "            article_sentences = [] # return empty list\n",
    "    finally:\n",
    "        return(article_sentences)\n",
    "\n",
    "\n",
    "# Split article content into list of one sentence per line\n",
    "def split_to_list(article):\n",
    "    \"\"\" article_object -> list\n",
    "    Takes a wikipedia article object and extracts the contents as text, splitting to one sentence per line \n",
    "    and removing some irrelevant punctuation and short sentences. Returns a list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(article.content, language=\"english\") # split article by paragraph\n",
    "    sentences_final = []\n",
    "    skip=False\n",
    "    skip_set = ('i.e.','e.g.')\n",
    "    min_sentence_len = 50\n",
    "    i=0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if skip==True: # skip if needed\n",
    "            skip=False\n",
    "            i=i+1\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            l = len(sentence)\n",
    "            if sentence[l-4:l] in skip_set: # if last four chars match anything in the skip set (e.g. or i.e.)\n",
    "                sentence = sentence+' '+sentences[i+1] # combine with next line\n",
    "                skip=True # skip the next line as we just added it on to this line\n",
    "\n",
    "            sentence = re.sub('\\[.+\\]', '', sentence) # remove anything in square brackets (mostly the pronunciation guide)\n",
    "            sentence = re.sub('(\\W);', '\\\\1', sentence)\n",
    "            sentence = re.sub('([a-z]{2,}\\.)([A-Z][a-z])', '\\\\1\\n\\\\2', sentence) # split the weird sentences with .New format\n",
    "            sentence = re.sub(':\\s(\\d|,|\\s|\\–){2,}', '', sentence) # remove lingering page numbers\n",
    "            new_sentences = sentence.split('\\n') # split multi-line paragraphs\n",
    "\n",
    "            for new_sentence in new_sentences:\n",
    "                if new_sentence=='': # remove blank lines\n",
    "                    continue\n",
    "                elif new_sentence[0]=='=': # remove headings\n",
    "                    continue\n",
    "                elif len(new_sentence)<min_sentence_len: # remove very short lines\n",
    "                    continue\n",
    "                elif new_sentence[-1]!='.': # must end with full stop\n",
    "                    continue\n",
    "                elif new_sentence.find('ISBN ')>0: # ignore lines with ISBNs\n",
    "                    continue\n",
    "                else:\n",
    "                    new_sentence = new_sentence.replace('\"','') # remove quotation marks\n",
    "                    sentences_final.append(new_sentence)\n",
    "            i=i+1\n",
    "\n",
    "    print('Loaded: '+article.title+', Sentences: '+str(len(sentences_final))) # number of sentences\n",
    "    return(sentences_final)\n",
    "\n",
    "\n",
    "# Save list of stentences from a given article to a file\n",
    "def save_sentences(sentences_list, filename, path):\n",
    "    save_path = path+filename+'.txt'\n",
    "    save_file = open(save_path, \"a\", encoding='utf-8')\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        #print(sentence)\n",
    "        save_file.writelines(sentence)\n",
    "        save_file.write('\\n')            \n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b3932-1165-40ee-ae40-5c6988bcef9c",
   "metadata": {},
   "source": [
    "### Functions for getting sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af5e65a-7b98-4b28-85ac-a3ae33dc3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_sim(embed_1, embed_2):\n",
    "    \"\"\" numpy_array, numpy_array -> float\n",
    "    Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "    \"\"\"\n",
    "    if np.dot(embed_1,embed_2) == 0:\n",
    "        similarity = 0 # don't normalise if similarity is zero\n",
    "    else:\n",
    "        similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "    return(similarity)\n",
    "\n",
    "\n",
    "# Get decontextualised transformer embedding for given word\n",
    "def transformer_embed_decontext(model, tokenizer, word, layer=0, embed_type='decontext'):\n",
    "    encoded_input = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "    model_output = model(**encoded_input)\n",
    "    word_embedding_raw = model_output.hidden_states[layer].detach().numpy()[0]\n",
    "     \n",
    "    if embed_type=='mean': # take the mean of all tokens\n",
    "        word_embedding = word_embedding_raw.mean(axis=0)\n",
    "    elif embed_type=='cls': # use the 'CLS' token\n",
    "        word_embedding = word_embedding_raw[0]\n",
    "    elif embed_type=='decontext': # take the mean of word tokens only\n",
    "        word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "\n",
    "    return(word_embedding)\n",
    "\n",
    "\n",
    "# Get contextualised transformer embedding for single word over the entire corpus\n",
    "def transformer_embed_context(model, tokenizer, target_word, sentence_corpus, count_limit=100, layer=0):\n",
    "    embeddings_storage = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    count=0\n",
    "    for sentence in sentence_corpus:\n",
    "\n",
    "        lemmatised_sentence_list = []\n",
    "        lemmatised_sentence_dict = {}\n",
    "        for original_word in sentence.split():\n",
    "            fixed_original_word = re.sub('[,;.?!\\)\\(]', '', original_word) # remove punctuation\n",
    "            lemmatised_word = lemmatizer.lemmatize(fixed_original_word) # lematise all words in sentence\n",
    "            lemmatised_sentence_list.append(lemmatised_word)\n",
    "            lemmatised_sentence_dict[lemmatised_word] = original_word\n",
    "\n",
    "        if count>count_limit: # don't need more than 500 sentences\n",
    "            break\n",
    "        if target_word in lemmatised_sentence_list:\n",
    "            count=count+1 \n",
    "            encoded_input = tokenizer(sentence, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "            model_output = model(**encoded_input)\n",
    "            sent_embedding_raw = model_output.hidden_states[12].detach().numpy()[0] # get sentence embeddings\n",
    "\n",
    "            original_word = lemmatised_sentence_dict[target_word] # undo lemmatisation of matching word\n",
    "            encoded_target_word = tokenizer(original_word, return_tensors='pt') #pt = pytorch\n",
    "            target_code = int(encoded_target_word.input_ids[0][1]) # get token code for target word \n",
    "            target_index = list(np.array(encoded_input.input_ids[0])).index(target_code) # look for token code in sentence to find the right word embedding\n",
    "            embeddings_storage.append(sent_embedding_raw[target_index]) # get embedding of target word from sentence\n",
    "    \n",
    "    return(np.array(embeddings_storage), count)\n",
    "\n",
    "\n",
    "# Load word similarity dataset\n",
    "def load_sim_dataset(model):\n",
    "    path = path_base+'Word Similarity Data\\Word Similarities Final\\\\'\n",
    "    filename = path+model+'.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    wordpairs = [None]*len(lines) # initialise storage\n",
    "    ratings = [None]*len(lines)\n",
    "    i=0\n",
    "    for line in lines:\n",
    "        line = line.strip() # remove new line chars\n",
    "        wordpairs[i] = line.split() # split at any whitespace chars\n",
    "        ratings[i] = float(wordpairs[i][2])\n",
    "        i=i+1\n",
    "    ratings = np.array(ratings)\n",
    "\n",
    "    return(wordpairs, ratings)\n",
    "\n",
    "\n",
    "# Save contextual embeddings for a given word to a new file for each word\n",
    "def save_embeddings_word(word, word_embeddings, layer, path):\n",
    "    save_path = path+word+'_'+str(layer)+'.txt'\n",
    "    with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "        final_string = str(word_embeddings)[2:-1] # don't include brackets in string\n",
    "        save_file.writelines(final_string)\n",
    "        save_file.write('\\n')\n",
    "\n",
    "\n",
    "# Function to return a lemmatised list and dictionary for a given sentence\n",
    "def lemmatise_sent(sentence):\n",
    "    lemmatised_sentence_list = []\n",
    "    lemmatised_sentence_dict = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for original_word in sentence.split():\n",
    "        fixed_original_word = re.sub('[,;.?!\\)\\(]', '', original_word) # remove punctuation\n",
    "        lemmatised_word = lemmatizer.lemmatize(fixed_original_word, wordnet.VERB) # lematise all words in sentence\n",
    "        lemmatised_sentence_list.append(lemmatised_word)\n",
    "        lemmatised_sentence_dict[lemmatised_word] = original_word\n",
    "    return(lemmatised_sentence_list, lemmatised_sentence_dict)\n",
    "\n",
    "\n",
    "# get encoding number for a specific word\n",
    "def get_word_code(word):\n",
    "    encoded_word = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "    word_code = int(encoded_word.input_ids[0][1]) # get token code for target word\n",
    "    return(word_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8dd3a-da1b-4eda-8723-5698e4af3f79",
   "metadata": {},
   "source": [
    "### Load list of articles and save the sentences to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2617de99-a9af-401d-8a2d-2b8d9253ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 10001 articles loaded\n"
     ]
    }
   ],
   "source": [
    "# Load list of wikipedia articles to use\n",
    "titles_file = path_base+'Corpus Data\\Wikipedia 10k corpus\\\\article_list.txt'\n",
    "article_titles_pd = pd.read_table(titles_file, index_col=0, header=None, quoting=csv.QUOTE_NONE, skip_blank_lines=True)\n",
    "article_titles_list = article_titles_pd.index.values\n",
    "print('List of '+str(len(article_titles_list))+' articles loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae0d37be-1d79-47a0-8fc9-34e2cee49a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Tiger II, Sentences: 169\n"
     ]
    }
   ],
   "source": [
    "# Trial loading and printing article\n",
    "article_title = 'Königstiger'\n",
    "if check_wiki_article(article_title):\n",
    "    article_content = load_wiki_article(article_title)\n",
    "else:\n",
    "    print('not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d44ce9f-e844-49d5-879a-8356b795d79f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Lillian Gish, Sentences: 139\n",
      "Loaded: Buster Keaton, Sentences: 246\n",
      "Loaded: Harold Lloyd, Sentences: 155\n",
      "Loaded: Mary Pickford, Sentences: 221\n",
      "Loaded: Gloria Swanson, Sentences: 236\n",
      "Loaded: Asta Nielsen, Sentences: 78\n",
      "Loaded: Fred Astaire, Sentences: 267\n"
     ]
    }
   ],
   "source": [
    "# Load plain text of all wiki articles from list and save sentences to file\n",
    "save_path = path_base+'/Corpus Data/'\n",
    "for article_title in article_titles_list:\n",
    "    sentences_list = load_wiki_article(article_title)\n",
    "    save_sentences(sentences_list, 'full_corpus_4', save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313526e-3a99-4241-bc16-92cbc45c59db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analyse the sentences and get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f523326-9dad-491b-96bb-f7f590049fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002787 sentences loaded\n"
     ]
    }
   ],
   "source": [
    "# Load full corpus of sentences from file\n",
    "path = path_base+'Corpus Data/Wikipedia 10k corpus/full_corpus.txt'\n",
    "sentence_corpus_pd = pd.read_table(path, index_col=0, header=None, sep=\"\\n\", quoting=csv.QUOTE_NONE, skip_blank_lines=True)\n",
    "sentence_corpus_list = sentence_corpus_pd.index.values\n",
    "print(str(len(sentence_corpus_list))+' sentences loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba751b47-3b93-42d8-b944-9633d62e26b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ernie-2.0-en model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load transformer model\n",
    "model_name = 'ernie-2.0-en'\n",
    "path = path_base+'Sentence Embeddings//'+model_name\n",
    "logging.set_verbosity_error() # turn off annoying model initialisation warning\n",
    "config_state = AutoConfig.from_pretrained(path, output_hidden_states=True) # get hidden states\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path, config=config_state)\n",
    "print(model_name+' model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc1533c-e5f7-496f-b6f9-7ec6bc62a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_nouns vocab loaded\n",
      "5824 words\n"
     ]
    }
   ],
   "source": [
    "# Load vocab set\n",
    "dataset_name = 'all_nouns'\n",
    "dataset, _ = load_sim_dataset('combined_dataset_nouns')\n",
    "vocab = []\n",
    "for word_pair in dataset:\n",
    "    vocab.append(word_pair[0])\n",
    "    vocab.append(word_pair[1])\n",
    "vocab_set = list(set(vocab))\n",
    "vocab_set.sort()\n",
    "print(dataset_name+' vocab loaded')\n",
    "print(str(len(vocab_set))+' words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a2e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise vocab counter\n",
    "vocab_storage_count = Counter()\n",
    "path = path_base+'Word Embeddings\\Ernie Contextual Verbs\\\\'\n",
    "\n",
    "for word in list(vocab_set): # looping over words in vocab set\n",
    "    filename = path+word+'_1.txt'\n",
    "    if os.path.isfile(filename): # check if that word has any saved embeddings\n",
    "        with open(filename) as file:\n",
    "            lines = [line.rstrip('\\n') for line in file]\n",
    "            vocab_storage_count[word] = len(lines) # get count of number of embeddings saved\n",
    "    else:\n",
    "        vocab_storage_count[word] = 0 # no embeddings saved yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb41182-8d96-4911-9655-94b70a4b32a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|██████▊                                 | ▄▆█ 336397/2002787 [17%] in 4:09:04 (22.5/s, eta: 20:33:46) 18547/2002787 [1%] in 15:15 (20.3/s, eta: 27:11:21) in 35:07 (20.5/s, eta: 26:36:50) (20.4/s, eta: 26:21:32) in 1:02:38 (20.7/s, eta: 25:48:29) (21.2/s, eta: 24:50:13)  ▅▃▁ 115060/2002787 [6%] in 1:29:45 (21.4/s, eta: 24:32:37)  ▅▇▇ 139488/2002787 [7%] in 1:47:19 (21.7/s, eta: 23:53:33) in 2:48:08 (22.2/s, eta: 22:13:12) ▃▁▃ 229093/2002787 [11%] in 2:51:40 (22.2/s, eta: 22:09:07)  ▆█▆ 230097/2002787 [11%] in 2:52:26 (22.2/s, eta: 22:08:28) 232277/2002787 [12%] in 2:53:59 (22.3/s, eta: 22:06:12) (22.3/s, eta: 21:48:46) (22.4/s, eta: 21:26:50) 278053/2002787 [14%] in 3:27:18 (22.4/s, eta: 21:25:52)  279805/2002787 [14%] in 3:28:33 (22.4/s, eta: 21:24:13) in 3:47:49 (22.4/s, eta: 20:59:16) ▆▄▂ 319652/2002787 [16%] in 3:57:04 (22.5/s, eta: 20:48:18) ▃▁▃ 329118/2002787 [16%] in 4:03:42 (22.5/s, eta: 20:39:19) (22.5/s, eta: 20:34:52) "
     ]
    }
   ],
   "source": [
    "# Get contextual embeddings for each word in vocab set and save each to separate file\n",
    "start_num = 0 # number to start at if not the beginning\n",
    "count = start_num\n",
    "count_limit = 100 # max number of contextualised embeddings per word\n",
    "\n",
    "with alive_bar(len(sentence_corpus_list)-start_num, force_tty=True) as bar: # progress bar to show progress\n",
    "\n",
    "    for sentence in sentence_corpus_list[start_num:]:\n",
    "\n",
    "        # get list of vocab that we still need to get embeddings for (limit not reached)\n",
    "        remaining_vocab = [word for word in vocab_set if vocab_storage_count[word]<count_limit]\n",
    "\n",
    "        # get lemmatiseed list of words in sentence\n",
    "        sent_encoded_input = tokenizer(sentence, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "        encoded_word_ids = np.array(sent_encoded_input.input_ids[0])\n",
    "        sent_model_output = model(**sent_encoded_input)\n",
    "        lemmatised_sentence_list, lemmatised_sentence_dict = lemmatise_sent(sentence) # get lemmatised version of the sentence\n",
    "\n",
    "        # get embeddings for all words from the vocab set\n",
    "        for lemmatised_word in list(set(lemmatised_sentence_list) & set(remaining_vocab)): # get words in the vocab set\n",
    "            original_word = lemmatised_sentence_dict[lemmatised_word] # undo lemmatisation of matching word\n",
    "            encoded_word = tokenizer(original_word, return_tensors='pt')\n",
    "            word_code = int(encoded_word.input_ids[0][1]) # get token code for target word\n",
    "            target_index = list(np.array(sent_encoded_input.input_ids[0])).index(word_code) # look for token code in sentence to find the right word embedding\n",
    "\n",
    "            # get embeddings for each layer of network\n",
    "            for layer in range(1,13):\n",
    "                sent_embedding_raw = sent_model_output.hidden_states[layer].detach().numpy()[0] # get sentence embeddings\n",
    "                word_embedding = sent_embedding_raw[target_index]\n",
    "                save_embeddings_word(lemmatised_word, word_embedding, layer, path) # save embeddings to file by layer\n",
    "\n",
    "            vocab_storage_count[lemmatised_word] += 1\n",
    "\n",
    "        bar() # needed for progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654609b0-c2e2-4cb3-b0a4-aff01ea3166d",
   "metadata": {},
   "source": [
    "### Put together full set of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f725a4d1-1191-4e31-af4f-30d847a2423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dictionary of all contextualised word embeddings\n",
    "word_embed_dict = {}\n",
    "path = path_base+'Word Embeddings\\Ernie Contextual Nouns\\\\'\n",
    "for layer in range(1,13):\n",
    "    layer_dict = {}\n",
    "    for word in vocab_set: # read all words in vocab set\n",
    "        filename = path+word+'_'+str(layer)+'.txt' # get the file\n",
    "        try:\n",
    "            with open(filename) as file:\n",
    "                np_lines = np.loadtxt(file)\n",
    "                layer_dict[word] = np_lines\n",
    "        except: # use non-contextual embedding if no contextual available\n",
    "            embedding = transformer_embed_decontext(model, tokenizer, word, layer=0)\n",
    "            layer_dict[word] = embedding\n",
    "    word_embed_dict[layer] = layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f6d1513-1cde-4328-8e80-02c204f68922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw contextualised embeddings to a single text file\n",
    "for layer in range(1,13):\n",
    "    for word in word_embed_dict[layer].keys():\n",
    "        if len(np.ndarray.flatten(word_embed_dict[layer][word])) > 800: # only if we only have multiple embeddings\n",
    "            contextual_embedding = np.mean(word_embed_dict[layer][word], axis=0) # average over all saved embeddings\n",
    "        else:\n",
    "            contextual_embedding = word_embed_dict[layer][word]\n",
    "        save_path = path+'\\contextual_embeddings_layer_'+str(layer)+'.txt'\n",
    "\n",
    "        with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "            final_string = word+' '+str(contextual_embedding)[2:-1] # don't include brackets in string\n",
    "            save_file.writelines(final_string)\n",
    "            save_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9684558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save normalised transformer embeddings; see paper 'all bark and no bite'\n",
    "for layer in range(1,13):\n",
    "\n",
    "    # Open file with unnormalised embeddings\n",
    "    filename = path+'\\contextual_embeddings_layer_'+str(layer)+'.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    # Load values into dictionary\n",
    "    model_dict = {}\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        embedding_list = [float(x) for x in word_list[1:-1]]\n",
    "        embedding_np = np.array(embedding_list)\n",
    "        model_dict[word] = embedding_np\n",
    "\n",
    "    # Convert to numpy array\n",
    "    first_key = list(model_dict.keys())[0]\n",
    "    length = len(model_dict[first_key])\n",
    "    model_np = np.empty((0,length), float)\n",
    "    for word in model_dict.keys():\n",
    "        model_np = np.vstack([model_np, model_dict[word]])\n",
    "\n",
    "    # Normalise array\n",
    "    mean_np = np.mean(model_np,axis=0)\n",
    "    std_np = np.std(model_np, axis=1)\n",
    "    mean_tp_np = np.transpose(model_np - mean_np)\n",
    "    model_final_np = np.transpose(mean_tp_np/std_np)\n",
    "\n",
    "    # Save normalised embeddings to new file\n",
    "    save_path = path+'contextual_embeddings_layer_normalised_'+str(layer)+'.txt'\n",
    "    i=0\n",
    "    with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "        for word in model_dict.keys():\n",
    "            final_string = word+' '+str(model_final_np[i,:])[1:-1] # remove brackets from numpy\n",
    "            save_file.writelines(final_string)\n",
    "            save_file.write('\\n')\n",
    "            i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04072a-ffb7-4675-954d-28de669eeb79",
   "metadata": {},
   "source": [
    "### Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d89e941-a30e-4406-b8d4-0e7269255abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 101 2182 2003 1037 3231 6251 1012  102]\n"
     ]
    }
   ],
   "source": [
    "# Print the encoding ids for a whole sentence\n",
    "sentence = 'here is a test sentence.'\n",
    "encoded_input = tokenizer(sentence, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "model_output = model(**encoded_input)\n",
    "sent_embedding_raw = model_output.hidden_states[12].detach().numpy()[0] # get sentence embeddings\n",
    "encoded_ids = np.array(encoded_input.input_ids[0])\n",
    "print(encoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "16103b36-06e1-492c-854e-c6686b2f869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19416906\n"
     ]
    }
   ],
   "source": [
    "# Test single words in BERT\n",
    "embed_1 = transformer_embed_decontext(model, tokenizer, 'machine', layer=0)\n",
    "embed_2 = transformer_embed_decontext(model, tokenizer, 'industry', layer=0)\n",
    "print(cosine_sim(embed_1,embed_2))\n",
    "\n",
    "# Test single words in BERT with context\n",
    "embed_all_1, count_1 = transformer_embed_context(model, tokenizer, 'machine', count_limit=100, layer=12)\n",
    "embed_all_2, count_2 = transformer_embed_context(model, tokenizer, 'industry', count_limit=100, layer=12)\n",
    "mean_embed_1 = np.mean(embed_all_1, axis=0)\n",
    "mean_embed_2 = np.mean(embed_all_2, axis=0)\n",
    "print(cosine_sim(mean_embed_1, mean_embed_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476100e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
