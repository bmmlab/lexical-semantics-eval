{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9c5eed-3731-4a0b-8ce1-6abcbc22ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to extract example sentences from Wikipedia articles\n",
    "## James Fodor 2022\n",
    "## Python 3.8\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import logging\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Set print option for numpy, needed for saving embeddings\n",
    "np.set_printoptions(precision=4, threshold=10000, linewidth=10000, suppress=True, floatmode='fixed')\n",
    "\n",
    "# Define base path location for data\n",
    "path_base = 'D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\\\'\n",
    "\n",
    "# Get wordnet to work\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.data import path # need to specify the location of the nltk data\n",
    "path.append(path_base+\"\\Frames and Structured Data\\\\FrameNet\\\\nltk_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "271b3932-1165-40ee-ae40-5c6988bcef9c",
   "metadata": {},
   "source": [
    "### Functions for getting sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af5e65a-7b98-4b28-85ac-a3ae33dc3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_sim(embed_1, embed_2):\n",
    "    \"\"\" numpy_array, numpy_array -> float\n",
    "    Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "    \"\"\"\n",
    "    if np.dot(embed_1,embed_2) == 0:\n",
    "        similarity = 0 # don't normalise if similarity is zero\n",
    "    else:\n",
    "        similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "    return(similarity)\n",
    "\n",
    "\n",
    "# Get decontextualised transformer embedding for given word\n",
    "def transformer_embed_decontext(model, tokenizer, word, layer=0, embed_type='decontext'):\n",
    "    \"\"\" pytorch_model, pytorch_tokenzier, str, int, str -> np_array\n",
    "    Extracts a word embedding given a pythorch model and tokenizer. Three modes of operation\n",
    "    depending on how to get the word embedding.\n",
    "    \"\"\"\n",
    "    encoded_input = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "    model_output = model(**encoded_input)\n",
    "    word_embedding_raw = model_output.hidden_states[layer].detach().numpy()[0]\n",
    "     \n",
    "    if embed_type=='mean': # take the mean of all tokens\n",
    "        word_embedding = word_embedding_raw.mean(axis=0)\n",
    "    elif embed_type=='cls': # use the 'CLS' token\n",
    "        word_embedding = word_embedding_raw[0]\n",
    "    elif embed_type=='decontext': # take the mean of word tokens only\n",
    "        word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "\n",
    "    return(word_embedding)\n",
    "\n",
    "\n",
    "# Load word similarity dataset\n",
    "def load_sim_dataset(dataset):\n",
    "    \"\"\" str -> (list_str, np_array)\n",
    "    Loads a dataset of word similarities, returning the word pairs and similarity ratings.\n",
    "    \"\"\"\n",
    "    path = path_base+'Word Similarity Data\\Word Similarities Final\\\\'\n",
    "    filename = path+dataset+'.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    wordpairs = [None]*len(lines) # initialise storage\n",
    "    ratings = [None]*len(lines)\n",
    "    i=0\n",
    "    for line in lines:\n",
    "        line = line.strip() # remove new line chars\n",
    "        wordpairs[i] = line.split() # split at any whitespace chars\n",
    "        ratings[i] = float(wordpairs[i][2])\n",
    "        i=i+1\n",
    "    ratings = np.array(ratings)\n",
    "\n",
    "    return(wordpairs, ratings)\n",
    "\n",
    "\n",
    "# Save contextual embeddings for a given word to a new file for each word\n",
    "def save_embeddings_word(word, word_embeddings, layer, path):\n",
    "    \"\"\" str, np_array, int, str -> None\n",
    "    Saves word_embeddings for specified word to a specified path.\n",
    "    \"\"\"\n",
    "    save_path = path+word+'_'+str(layer)+'.txt'\n",
    "    with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "        final_string = str(word_embeddings)[2:-1] # don't include brackets in string\n",
    "        save_file.writelines(final_string)\n",
    "        save_file.write('\\n')\n",
    "\n",
    "\n",
    "# Function to return a lemmatised list and dictionary for a given sentence\n",
    "def lemmatise_sent(sentence):\n",
    "    \"\"\" str -> list_str, dict\n",
    "    Takes in a sentence and returns a tokenised and lemmatised list and dictionary\n",
    "    for all words in the sentence.\n",
    "    \"\"\"\n",
    "    lemmatised_sentence_list = []\n",
    "    lemmatised_sentence_dict = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for original_word in sentence.split():\n",
    "        fixed_original_word = re.sub('[,;.?!\\)\\(]', '', original_word) # remove punctuation\n",
    "        lemmatised_word = lemmatizer.lemmatize(fixed_original_word, wordnet.VERB) # lematise all words in sentence\n",
    "        lemmatised_sentence_list.append(lemmatised_word)\n",
    "        lemmatised_sentence_dict[lemmatised_word] = original_word\n",
    "    return(lemmatised_sentence_list, lemmatised_sentence_dict)\n",
    "\n",
    "\n",
    "# get token number for a specific word\n",
    "def get_word_code(word):\n",
    "    encoded_word = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "    word_code = int(encoded_word.input_ids[0][1]) # get token code for target word\n",
    "    return(word_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9313526e-3a99-4241-bc16-92cbc45c59db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analyse the sentences and get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f523326-9dad-491b-96bb-f7f590049fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002787 sentences loaded\n"
     ]
    }
   ],
   "source": [
    "# Load full corpus of sentences from file\n",
    "filename = path_base+'Corpus Data/Wikipedia 10k corpus/full_corpus.txt'\n",
    "with open(filename, encoding='utf-8') as file:\n",
    "    sentence_corpus = [line.rstrip('\\n') for line in file]\n",
    "print(str(len(sentence_corpus))+' sentences loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba751b47-3b93-42d8-b944-9633d62e26b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ernie-2.0-base-en model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load transformer model\n",
    "model_name = 'ernie-2.0-base-en'\n",
    "path = path_base+'Sentence Embeddings//'+model_name\n",
    "logging.set_verbosity_error() # turn off annoying model initialisation warning\n",
    "config_state = AutoConfig.from_pretrained(path, output_hidden_states=True) # get hidden states\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path, config=config_state)\n",
    "print(model_name+' model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbc1533c-e5f7-496f-b6f9-7ec6bc62a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN-SIMLEX-999-VERB vocab loaded\n",
      "170 words\n"
     ]
    }
   ],
   "source": [
    "# Load vocab set\n",
    "dataset_name = 'EN-SIMLEX-999-VERB'\n",
    "dataset, _ = load_sim_dataset(dataset_name)\n",
    "vocab = []\n",
    "for word_pair in dataset:\n",
    "    vocab.append(word_pair[0])\n",
    "    vocab.append(word_pair[1])\n",
    "vocab_set = list(set(vocab))\n",
    "vocab_set.sort()\n",
    "print(dataset_name+' vocab loaded')\n",
    "print(str(len(vocab_set))+' words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28a2e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise vocab counter\n",
    "vocab_storage_count = Counter()\n",
    "save_path = '' # location to save word embeddings\n",
    "\n",
    "for word in list(vocab_set): # looping over words in vocab set\n",
    "    filename = path+word+'_1.txt'\n",
    "    if os.path.isfile(filename): # check if that word has any saved embeddings\n",
    "        with open(filename) as file:\n",
    "            lines = [line.rstrip('\\n') for line in file]\n",
    "            vocab_storage_count[word] = len(lines) # get count of number of embeddings saved\n",
    "    else:\n",
    "        vocab_storage_count[word] = 0 # no embeddings saved yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb41182-8d96-4911-9655-94b70a4b32a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get contextual embeddings for each word in vocab set and save each to separate file\n",
    "start_num = 0 # number to start at if not the beginning\n",
    "count = start_num\n",
    "count_limit = 100 # max number of contextualised embeddings per word\n",
    "\n",
    "# loop over all sentences in corpus\n",
    "for sentence in sentence_corpus[start_num:]:\n",
    "\n",
    "    # get list of vocab that we still need to get embeddings if limit not reached\n",
    "    remaining_vocab = [word for word in vocab_set if vocab_storage_count[word]<count_limit]\n",
    "\n",
    "    # get lemmatiseed list of words in sentence\n",
    "    sent_encoded_input = tokenizer(sentence, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "    encoded_word_ids = np.array(sent_encoded_input.input_ids[0])\n",
    "    sent_model_output = model(**sent_encoded_input)\n",
    "    lemmatised_sentence_list, lemmatised_sentence_dict = lemmatise_sent(sentence) # get lemmatised version of the sentence\n",
    "\n",
    "    # get embeddings for all words from the vocab set\n",
    "    for lemmatised_word in list(set(lemmatised_sentence_list) & set(remaining_vocab)): # get words in the vocab set\n",
    "        original_word = lemmatised_sentence_dict[lemmatised_word] # undo lemmatisation of matching word\n",
    "        encoded_word = tokenizer(original_word, return_tensors='pt')\n",
    "        word_code = int(encoded_word.input_ids[0][1]) # get token code for target word\n",
    "        target_index = list(np.array(sent_encoded_input.input_ids[0])).index(word_code) # look for token code in sentence to find the right word embedding\n",
    "\n",
    "        # get embeddings for each layer of network\n",
    "        for layer in range(1,13):\n",
    "            sent_embedding_raw = sent_model_output.hidden_states[layer].detach().numpy()[0] # get sentence embeddings\n",
    "            word_embedding = sent_embedding_raw[target_index]\n",
    "            save_embeddings_word(lemmatised_word, word_embedding, layer, save_path) # save embeddings to file by layer\n",
    "\n",
    "        vocab_storage_count[lemmatised_word] += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "654609b0-c2e2-4cb3-b0a4-aff01ea3166d",
   "metadata": {},
   "source": [
    "### Put together full set of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f725a4d1-1191-4e31-af4f-30d847a2423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dictionary of all contextualised word embeddings\n",
    "word_embed_dict = {}\n",
    "for layer in range(1,13):\n",
    "    layer_dict = {}\n",
    "    for word in vocab_set: # read all words in vocab set\n",
    "        filename = save_path+word+'_'+str(layer)+'.txt' # get the file\n",
    "        try:\n",
    "            with open(filename) as file:\n",
    "                np_lines = np.loadtxt(file)\n",
    "                layer_dict[word] = np_lines\n",
    "        except: # use non-contextual embedding if no contextual available\n",
    "            embedding = transformer_embed_decontext(model, tokenizer, word, layer=0)\n",
    "            layer_dict[word] = embedding\n",
    "    word_embed_dict[layer] = layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f6d1513-1cde-4328-8e80-02c204f68922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw contextualised embeddings to a single text file\n",
    "for layer in range(1,13):\n",
    "    for word in word_embed_dict[layer].keys():\n",
    "        if len(np.ndarray.flatten(word_embed_dict[layer][word])) > 800: # only if we only have multiple embeddings\n",
    "            contextual_embedding = np.mean(word_embed_dict[layer][word], axis=0) # average over all saved embeddings\n",
    "        else:\n",
    "            contextual_embedding = word_embed_dict[layer][word]\n",
    "        raw_embeds_file = save_path+'contextual_embeddings_layer_'+str(layer)+'.txt'\n",
    "\n",
    "        with open(raw_embeds_file, \"a\", encoding='utf-8') as file:\n",
    "            final_string = word+' '+str(contextual_embedding)[2:-1] # don't include brackets in string\n",
    "            file.writelines(final_string)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9684558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save normalised transformer embeddings; see paper 'all bark and no bite'\n",
    "for layer in range(1,13):\n",
    "\n",
    "    # Open file with unnormalised embeddings\n",
    "    raw_embeds_file = save_path+'contextual_embeddings_layer_'+str(layer)+'.txt'\n",
    "    with open(raw_embeds_file) as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    # Load values into dictionary\n",
    "    model_dict = {}\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        embedding_list = [float(x) for x in word_list[1:-1]]\n",
    "        embedding_np = np.array(embedding_list)\n",
    "        model_dict[word] = embedding_np\n",
    "\n",
    "    # Convert to numpy array\n",
    "    first_key = list(model_dict.keys())[0]\n",
    "    length = len(model_dict[first_key])\n",
    "    model_np = np.empty((0,length), float)\n",
    "    for word in model_dict.keys():\n",
    "        model_np = np.vstack([model_np, model_dict[word]])\n",
    "\n",
    "    # Normalise array\n",
    "    mean_np = np.mean(model_np,axis=0)\n",
    "    std_np = np.std(model_np, axis=1)\n",
    "    mean_tp_np = np.transpose(model_np - mean_np)\n",
    "    model_final_np = np.transpose(mean_tp_np/std_np)\n",
    "\n",
    "    # Save normalised embeddings to new file\n",
    "    norm_embeds_file = save_path+'contextual_embeddings_layer_normalised_'+str(layer)+'.txt'\n",
    "    i=0\n",
    "    with open(norm_embeds_file, \"a\", encoding='utf-8') as file:\n",
    "        for word in model_dict.keys():\n",
    "            final_string = word+' '+str(model_final_np[i,:])[1:-1] # remove brackets from numpy\n",
    "            file.writelines(final_string)\n",
    "            file.write('\\n')\n",
    "            i=i+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b04072a-ffb7-4675-954d-28de669eeb79",
   "metadata": {},
   "source": [
    "### Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d89e941-a30e-4406-b8d4-0e7269255abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 101 2182 2003 1037 3231 6251 1012  102]\n"
     ]
    }
   ],
   "source": [
    "# Print the encoding ids for a whole sentence\n",
    "sentence = 'here is a test sentence.'\n",
    "encoded_input = tokenizer(sentence, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "model_output = model(**encoded_input)\n",
    "sent_embedding_raw = model_output.hidden_states[12].detach().numpy()[0] # get sentence embeddings\n",
    "encoded_ids = np.array(encoded_input.input_ids[0])\n",
    "print(encoded_ids)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
