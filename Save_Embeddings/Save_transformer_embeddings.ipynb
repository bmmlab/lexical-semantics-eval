{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d9e7cb-ed72-4d1f-a8dc-fe0e82b820ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to extract example sentences from Wikipedia articles\n",
    "## James Fodor 2022\n",
    "## Python 3.8\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import logging\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# Turn off annoying model initialisation warning\n",
    "logging.set_verbosity_error() \n",
    "\n",
    "# Set print option for numpy, needed for saving embeddings\n",
    "np.set_printoptions(precision=4, threshold=10000, linewidth=10000, suppress=True, floatmode='fixed')\n",
    "\n",
    "# Define base path location for data\n",
    "path_base = 'D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbc34b3-ec91-4746-84cc-76737199d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Key functions\n",
    "\n",
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_sim(embed_1, embed_2):\n",
    "    \"\"\" numpy_array, numpy_array -> float\n",
    "    Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "    \"\"\"\n",
    "    if np.dot(embed_1,embed_2) == 0:\n",
    "        similarity = 0 # don't normalise if similarity is zero\n",
    "    else:\n",
    "        similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "        #similarity, _ = spearmanr(embed_1, embed_2)\n",
    "    return(similarity)\n",
    "\n",
    "# Get decontextualised transformer embedding for given word\n",
    "def transformer_embed_decontext(model_name, model, tokenizer, word, layer=0, embed_type='decontext'):\n",
    "    all_hidden_layers = []\n",
    "    \n",
    "    if model_name=='gpt2' or model_name=='gpt2-large': # gpt2 model uses a different code\n",
    "        encoded_input = tokenizer.encode(word, add_prefix_space=True, return_tensors='pt')\n",
    "        word_embedding_raw = model.transformer.wte.weight[encoded_input,:].detach().numpy()[0][0]\n",
    "        word_embedding = word_embedding_raw.reshape(-1)\n",
    "        \n",
    "    elif model_name=='t5-base' or model_name=='t5-large' or model_name=='comet-atomic' or model_name=='libert-2m': # also uses different code\n",
    "        encoded_input = tokenizer(word, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "        model_output = model(**encoded_input)\n",
    "        word_embedding_raw = model_output.last_hidden_state.detach().numpy()[0] # layer=0 here gives gibberish\n",
    "        \n",
    "        if embed_type=='mean': # take the mean of all tokens\n",
    "            word_embedding = word_embedding_raw.mean(axis=0)\n",
    "        elif embed_type=='cls': # use the 'CLS' token\n",
    "            word_embedding = word_embedding_raw[0]\n",
    "        elif embed_type=='decontext': # take the mean of word tokens only\n",
    "            word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "\n",
    "    else: #most models use this code\n",
    "        encoded_input = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "        model_output = model(**encoded_input)\n",
    "        word_embedding_raw = model_output.hidden_states[layer].detach().numpy()[0]\n",
    "        all_hidden_layers = model_output.hidden_states\n",
    "\n",
    "        if embed_type=='mean': # take the mean of all tokens\n",
    "            word_embedding = word_embedding_raw.mean(axis=0)\n",
    "        elif embed_type=='cls': # use the 'CLS' token\n",
    "            word_embedding = word_embedding_raw[0]\n",
    "        elif embed_type=='decontext': # take the mean of word tokens only\n",
    "            word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "\n",
    "    return(word_embedding, all_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9a0893-341a-47c0-987b-c4b387ef41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformer model\n",
    "model_name = 'ernie-2.0-base-en' #name of subfolder for model\n",
    "filename = path_base+'Sentence Embeddings//'+model_name\n",
    "\n",
    "if model_name=='gpt2' or model_name=='gpt2-large':\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(filename)\n",
    "    model = GPT2LMHeadModel.from_pretrained(filename)\n",
    "elif model_name=='t5-base' or model_name=='t5-large':\n",
    "    config_state = AutoConfig.from_pretrained(filename, output_hidden_states=True) # get hidden states\n",
    "    tokenizer = T5Tokenizer.from_pretrained(filename)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(filename, config=config_state)\n",
    "elif model_name=='libert-2m':\n",
    "    config_state = AutoConfig.from_pretrained(filename, output_hidden_states=True) # get hidden states\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained('kssteven/ibert-roberta-base') # needs a different tokenizer for some reason\n",
    "    model = AutoModel.from_pretrained(filename, config=config_state)\n",
    "else: # for BERT-based models\n",
    "    config_state = AutoConfig.from_pretrained(filename, output_hidden_states=True) # get hidden states\n",
    "    tokenizer = AutoTokenizer.from_pretrained(filename)\n",
    "    model = AutoModel.from_pretrained(filename, config=config_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e735d6-d516-41b3-bd89-b312026b7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save transformer single word embeddings (decontextualised)\n",
    "vocab_file = 'Vocab_lists/combined_corpus_66k.txt' # from combined vocab set\n",
    "vocab_path = path_base + vocab_file\n",
    "\n",
    "# Loop over all vocab in vocab file\n",
    "with open(vocab_path, 'rt', encoding='utf-8') as vocab_file:\n",
    "    for line in vocab_file:\n",
    "        word = line.strip()\n",
    "        try:\n",
    "            # Get word embedding\n",
    "            word_embed, all_hidden_layers = transformer_embed_decontext(model_name, model, tokenizer, word) \n",
    "            \n",
    "            # Save embeddings for all layers\n",
    "            for layer in np.arange(0,13): \n",
    "                word_embedding_raw = all_hidden_layers[layer].detach().numpy()[0]\n",
    "                word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "                embed_string = np.array_str(word_embedding) # convert np_array to string\n",
    "                final_string = (word+' '+embed_string[2:-1]) # add word to front of embed string\n",
    "                \n",
    "                save_path = path_base + 'Word Embeddings/'+model_name+'-layer-'+str(layer)+'.txt'\n",
    "                save_file = open(save_path, \"a\", encoding='utf-8')\n",
    "                save_file.writelines(final_string)\n",
    "                save_file.write('\\n')\n",
    "                save_file.close()\n",
    "        except:\n",
    "                continue # skip if we can't find the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4901a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save normalised transformer embeddings; see paper 'all bark and no bite'\n",
    "for layer in range(1,13):\n",
    "\n",
    "    # Open file with unnormalised embeddings\n",
    "    raw_embeds_file = save_path+'contextual_embeddings_layer_'+str(layer)+'.txt'\n",
    "    with open(raw_embeds_file) as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    # Load values into dictionary\n",
    "    model_dict = {}\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        embedding_list = [float(x) for x in word_list[1:-1]]\n",
    "        embedding_np = np.array(embedding_list)\n",
    "        model_dict[word] = embedding_np\n",
    "\n",
    "    # Convert to numpy array\n",
    "    first_key = list(model_dict.keys())[0]\n",
    "    length = len(model_dict[first_key])\n",
    "    model_np = np.empty((0,length), float)\n",
    "    for word in model_dict.keys():\n",
    "        model_np = np.vstack([model_np, model_dict[word]])\n",
    "\n",
    "    # Normalise array\n",
    "    mean_np = np.mean(model_np,axis=0)\n",
    "    std_np = np.std(model_np, axis=1)\n",
    "    mean_tp_np = np.transpose(model_np - mean_np)\n",
    "    model_final_np = np.transpose(mean_tp_np/std_np)\n",
    "\n",
    "    # Save normalised embeddings to new file\n",
    "    norm_embeds_file = save_path+'contextual_embeddings_layer_normalised_'+str(layer)+'.txt'\n",
    "    i=0\n",
    "    with open(norm_embeds_file, \"a\", encoding='utf-8') as file:\n",
    "        for word in model_dict.keys():\n",
    "            final_string = word+' '+str(model_final_np[i,:])[1:-1] # remove brackets from numpy\n",
    "            file.writelines(final_string)\n",
    "            file.write('\\n')\n",
    "            i=i+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
