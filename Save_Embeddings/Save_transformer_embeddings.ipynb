{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d9e7cb-ed72-4d1f-a8dc-fe0e82b820ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Spring env\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import logging\n",
    "from transformers import RobertaTokenizerFast\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "logging.set_verbosity_error() # turn off annoying model initialisation warning\n",
    "np.set_printoptions(precision=4, threshold=100000, linewidth=100000, suppress=True, floatmode='fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cbc34b3-ec91-4746-84cc-76737199d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_sim(embed_1, embed_2):\n",
    "    \"\"\" numpy_array, numpy_array -> float\n",
    "    Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "    \"\"\"\n",
    "    if np.dot(embed_1,embed_2) == 0:\n",
    "        similarity = 0 # don't normalise if similarity is zero\n",
    "    else:\n",
    "        similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "        #similarity, _ = spearmanr(embed_1, embed_2)\n",
    "    return(similarity)\n",
    "\n",
    "# Get decontextualised transformer embedding for given word\n",
    "def transformer_embed_decontext(model_name, model, tokenizer, word, layer=0, embed_type='decontext'):\n",
    "    all_hidden_layers = []\n",
    "    \n",
    "    if model_name=='gpt2' or model_name=='gpt2-large': # gpt2 model uses a different code\n",
    "        encoded_input = tokenizer.encode(word, add_prefix_space=True, return_tensors='pt')\n",
    "        word_embedding_raw = model.transformer.wte.weight[encoded_input,:].detach().numpy()[0][0]\n",
    "        word_embedding = word_embedding_raw.reshape(-1)\n",
    "        \n",
    "    elif model_name=='t5-base' or model_name=='t5-large' or model_name=='comet-atomic' or model_name=='libert-2m': # also uses different code\n",
    "        encoded_input = tokenizer(word, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "        model_output = model(**encoded_input)\n",
    "        word_embedding_raw = model_output.last_hidden_state.detach().numpy()[0] # layer=0 here gives gibberish\n",
    "        \n",
    "        if embed_type=='mean': # take the mean of all tokens\n",
    "            word_embedding = word_embedding_raw.mean(axis=0)\n",
    "        elif embed_type=='cls': # use the 'CLS' token\n",
    "            word_embedding = word_embedding_raw[0]\n",
    "        elif embed_type=='decontext': # take the mean of word tokens only\n",
    "            word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "\n",
    "    else: #most models use this code\n",
    "        encoded_input = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "        model_output = model(**encoded_input)\n",
    "        word_embedding_raw = model_output.hidden_states[layer].detach().numpy()[0]\n",
    "        all_hidden_layers = model_output.hidden_states\n",
    "\n",
    "        if embed_type=='mean': # take the mean of all tokens\n",
    "            word_embedding = word_embedding_raw.mean(axis=0)\n",
    "        elif embed_type=='cls': # use the 'CLS' token\n",
    "            word_embedding = word_embedding_raw[0]\n",
    "        elif embed_type=='decontext': # take the mean of word tokens only\n",
    "            word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "\n",
    "    return(word_embedding, all_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9a0893-341a-47c0-987b-c4b387ef41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformer model\n",
    "model_name = 'ernie-2.0-en' #name of subfolder for model\n",
    "file_base = 'D:/Study and Projects/School Work/Year 25 - PhD 1/Data/Sentence Embeddings//'\n",
    "filename = file_base+model_name\n",
    "\n",
    "if model_name=='gpt2' or model_name=='gpt2-large':\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(filename)\n",
    "    model = GPT2LMHeadModel.from_pretrained(filename)\n",
    "elif model_name=='t5-base' or model_name=='t5-large':\n",
    "    config_state = AutoConfig.from_pretrained(filename, output_hidden_states=True) # get hidden states\n",
    "    tokenizer = T5Tokenizer.from_pretrained(filename)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(filename, config=config_state)\n",
    "elif model_name=='libert-2m':\n",
    "    config_state = AutoConfig.from_pretrained(filename, output_hidden_states=True) # get hidden states\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained('kssteven/ibert-roberta-base') # needs a different tokenizer for some reason\n",
    "    model = AutoModel.from_pretrained(filename, config=config_state)\n",
    "else: # for BERT-based models\n",
    "    config_state = AutoConfig.from_pretrained(filename, output_hidden_states=True) # get hidden states\n",
    "    tokenizer = AutoTokenizer.from_pretrained(filename)\n",
    "    model = AutoModel.from_pretrained(filename, config=config_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e735d6-d516-41b3-bd89-b312026b7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save transformer single word embeddings (decontextualised)\n",
    "folder_loc= 'D:/Study and Projects/School Work/Year 25 - PhD 1/Data//'\n",
    "vocab_file = 'Corpus Data/Key vocab/combined_corpus_66k.txt' # from combined vocab set\n",
    "vocab_path = folder_loc + vocab_file\n",
    "\n",
    "# Loop over all vocab in vocab file\n",
    "with open(vocab_path, 'rt', encoding='utf-8') as vocab_file:\n",
    "    for line in vocab_file:\n",
    "        word = line.strip()\n",
    "        try:\n",
    "            # Get word embedding\n",
    "            word_embed, all_hidden_layers = transformer_embed_decontext(model_name, model, tokenizer, word) \n",
    "            \n",
    "            # Save embeddings for all layers\n",
    "            for layer in np.arange(0,13): \n",
    "                word_embedding_raw = all_hidden_layers[layer].detach().numpy()[0]\n",
    "                word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "                embed_string = np.array_str(word_embedding) # convert np_array to string\n",
    "                final_string = (word+' '+embed_string[2:-1]) # add word to front of embed string\n",
    "                \n",
    "                save_path = folder_loc + 'Word Embeddings/'+model_name+'-layer-'+str(layer)+'.txt'\n",
    "                save_file = open(save_path, \"a\", encoding='utf-8')\n",
    "                save_file.writelines(final_string)\n",
    "                save_file.write('\\n')\n",
    "                save_file.close()\n",
    "        except:\n",
    "                continue # skip if we can't find the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4901a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save normalised transformer embeddings; see paper 'all bark and no bite'\n",
    "folder_loc= 'D:/Study and Projects/School Work/Year 25 - PhD 1/Data/Word Embeddings/ConceptNet Embeddings//'\n",
    "\n",
    "for layer in range(0,1):\n",
    "\n",
    "    # Open file with unnormalised embeddings\n",
    "    filename = folder_loc+'\\ernie-2.0-en-layer-'+str(layer)+'.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    # Load values into dictionary\n",
    "    model_dict = {}\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        embedding_list = [float(x) for x in word_list[1:-1]]\n",
    "        embedding_np = np.array(embedding_list)\n",
    "        model_dict[word] = embedding_np\n",
    "\n",
    "    # Convert to numpy array\n",
    "    first_key = list(model_dict.keys())[0]\n",
    "    length = len(model_dict[first_key])\n",
    "    model_np = np.empty((0,length), float)\n",
    "    for word in model_dict.keys():\n",
    "        model_np = np.vstack([model_np, model_dict[word]])\n",
    "\n",
    "    # Normalise array\n",
    "    mean_np = np.mean(model_np,axis=0)\n",
    "    std_np = np.std(model_np, axis=1)\n",
    "    mean_tp_np = np.transpose(model_np - mean_np)\n",
    "    model_final_np = np.transpose(mean_tp_np/std_np)\n",
    "\n",
    "    # Save normalised embeddings to new file\n",
    "    save_path = folder_loc+'ernie_normalised_layer_'+str(layer)+'.txt'\n",
    "    i=0\n",
    "    with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "        for word in model_dict.keys():\n",
    "            final_string = word+' '+str(model_final_np[i,:])[1:-1] # remove brackets from numpy\n",
    "            save_file.writelines(final_string)\n",
    "            save_file.write('\\n')\n",
    "            i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36fb4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\Word Similarity Data\\Word Similarities Final\\EN-SimVerb-3500.txt'\n",
    "simverb_data = np.loadtxt(path,  delimiter='\\t', dtype='str', encoding='utf-8')\n",
    "\n",
    "words_1 = simverb_data[:,0]\n",
    "words_2 = simverb_data[:,1]\n",
    "expr_sims = simverb_data[:,2].astype('float')\n",
    "context_sims = []\n",
    "base_sims = []\n",
    "\n",
    "for word_pair in simverb_data:\n",
    "    word_1 = word_pair[0]\n",
    "    word_2 = word_pair[1]\n",
    "\n",
    "    # Code for testing\n",
    "    sentence_1 = word_1+', '+word_2\n",
    "    encoded_input_1 = tokenizer(sentence_1, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "    model_output = model(**encoded_input_1)\n",
    "    sent_embed_1 = model_output.hidden_states[0].detach().numpy()[0]\n",
    "    # sent_embedding_raw = model_output.last_hidden_state.detach().numpy()[0]\n",
    "    # print('token ids:',encoded_input_1['input_ids'][0])\n",
    "    context_sims.append(cosine_sim(sent_embed_1[1],sent_embed_1[3]))\n",
    "\n",
    "    sentence_2 = word_1\n",
    "    encoded_input_2 = tokenizer(sentence_2, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "    model_output = model(**encoded_input_2)\n",
    "    sent_embed_2 = model_output.hidden_states[0].detach().numpy()[0]\n",
    "    # sent_embedding_raw = model_output.last_hidden_state.detach().numpy()[0]\n",
    "    # print('token ids:',encoded_input_2['input_ids'][0])\n",
    "\n",
    "    sentence_3 = word_2\n",
    "    encoded_input_3 = tokenizer(sentence_3, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "    model_output = model(**encoded_input_3)\n",
    "    sent_embed_3 = model_output.hidden_states[0].detach().numpy()[0]\n",
    "    # sent_embedding_raw = model_output.last_hidden_state.detach().numpy()[0]\n",
    "    # print('token ids:',encoded_input_3['input_ids'][0])\n",
    "\n",
    "    base_sims.append(cosine_sim(sent_embed_2[1],sent_embed_3[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5880b2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000, 0.3461],\n",
       "       [0.3461, 1.0000]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(expr_sims,base_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e38c95e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000, 0.3356],\n",
       "       [0.3356, 1.0000]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(expr_sims,context_sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
