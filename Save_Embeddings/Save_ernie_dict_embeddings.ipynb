{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f9c5eed-3731-4a0b-8ce1-6abcbc22ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for calculating Dictionary Sense Embeddings using ERNIE transformer\n",
    "## James Fodor 2022\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import logging\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Set print option for numpy\n",
    "np.set_printoptions(precision=4, threshold=1000, linewidth=10000, suppress=True, floatmode='fixed')\n",
    "\n",
    "# Define base path location for data\n",
    "path_base = 'D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\\\'\n",
    "\n",
    "# Get wordnet to work\n",
    "from nltk.data import path # need to specify the location of the nltk data\n",
    "path.append(path_base+\"\\Frames and Structured Data\\FrameNet\\\\nltk_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "271b3932-1165-40ee-ae40-5c6988bcef9c",
   "metadata": {},
   "source": [
    "### Functions for getting sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af5e65a-7b98-4b28-85ac-a3ae33dc3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_sim(embed_1, embed_2):\n",
    "    \"\"\" numpy_array, numpy_array -> float\n",
    "    Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "    \"\"\"\n",
    "    if np.dot(embed_1,embed_2) == 0:\n",
    "        similarity = 0 # don't normalise if similarity is zero\n",
    "    else:\n",
    "        similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "    return(similarity)\n",
    "\n",
    "\n",
    "# Get decontextualised transformer embedding for given word\n",
    "def transformer_embed_decontext(model, tokenizer, word, layer=0, embed_type='decontext'):\n",
    "    encoded_input = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "    model_output = model(**encoded_input)\n",
    "    word_embedding_raw = model_output.hidden_states[layer].detach().numpy()[0]\n",
    "     \n",
    "    if embed_type=='mean': # take the mean of all tokens\n",
    "        word_embedding = word_embedding_raw.mean(axis=0)\n",
    "    elif embed_type=='cls': # use the 'CLS' token\n",
    "        word_embedding = word_embedding_raw[0]\n",
    "    elif embed_type=='decontext': # take the mean of word tokens only\n",
    "        word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "\n",
    "    return(word_embedding)\n",
    "\n",
    "\n",
    "# Get contextualised transformer embedding for single word over the entire corpus\n",
    "def transformer_embed_context(model, tokenizer, target_word, sentence_corpus, count_limit=100, layer=0):\n",
    "    embeddings_storage = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    count=0\n",
    "    for sentence in sentence_corpus:\n",
    "\n",
    "        lemmatised_sentence_list = []\n",
    "        lemmatised_sentence_dict = {}\n",
    "        for original_word in sentence.split():\n",
    "            fixed_original_word = re.sub('[‘`’\\\"\\',;.?!\\)\\(]', '', original_word) # remove punctuation\n",
    "            lemmatised_word = lemmatizer.lemmatize(fixed_original_word) # lematise all words in sentence\n",
    "            lemmatised_sentence_list.append(lemmatised_word)\n",
    "            lemmatised_sentence_dict[lemmatised_word] = original_word\n",
    "\n",
    "        if count>count_limit: # don't need more than 500 sentences\n",
    "            break\n",
    "        if target_word in lemmatised_sentence_list:\n",
    "            count=count+1 \n",
    "            encoded_input = tokenizer(sentence, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "            model_output = model(**encoded_input)\n",
    "            sent_embedding_raw = model_output.hidden_states[12].detach().numpy()[0] # get sentence embeddings\n",
    "\n",
    "            original_word = lemmatised_sentence_dict[target_word] # undo lemmatisation of matching word\n",
    "            encoded_target_word = tokenizer(original_word, return_tensors='pt') #pt = pytorch\n",
    "            target_code = int(encoded_target_word.input_ids[0][1]) # get token code for target word \n",
    "            target_index = list(np.array(encoded_input.input_ids[0])).index(target_code) # look for token code in sentence to find the right word embedding\n",
    "            embeddings_storage.append(sent_embedding_raw[target_index]) # get embedding of target word from sentence\n",
    "    \n",
    "    return(np.array(embeddings_storage), count)\n",
    "\n",
    "\n",
    "# Load word similarity dataset\n",
    "def load_sim_dataset(model):\n",
    "    path ='D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\Word Similarity Data\\Word Similarities Final\\\\'\n",
    "    filename = path+model+'.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    wordpairs = [None]*len(lines) # initialise storage\n",
    "    ratings = [None]*len(lines)\n",
    "    i=0\n",
    "    for line in lines:\n",
    "        line = line.strip() # remove new line chars\n",
    "        wordpairs[i] = line.split() # split at any whitespace chars\n",
    "        ratings[i] = float(wordpairs[i][2])\n",
    "        i=i+1\n",
    "    ratings = np.array(ratings)\n",
    "\n",
    "    return(wordpairs, ratings)\n",
    "\n",
    "\n",
    "# Save contextual embeddings for a given word to a new file for each word\n",
    "def save_embeddings_word(word, word_embeddings, layer, path):\n",
    "    save_path = path+word+'_'+str(layer)+'.txt'\n",
    "    with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "        final_string = str(word_embeddings)[2:-1] # don't include brackets in string\n",
    "        save_file.writelines(final_string)\n",
    "        save_file.write('\\n')\n",
    "\n",
    "\n",
    "# Function to return a lemmatised list and dictionary for a given sentence\n",
    "def lemmatise_sent(sentence):\n",
    "    lemmatised_sentence_list = []\n",
    "    lemmatised_sentence_dict = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for original_word in sentence.split():\n",
    "        fixed_original_word = re.sub('[‘`’\\\"\\',;.?!\\)\\(]', '', original_word) # remove punctuation\n",
    "        lemmatised_word = lemmatizer.lemmatize(fixed_original_word, wordnet.VERB) # lematise all words in sentence\n",
    "        lemmatised_sentence_list.append(lemmatised_word)\n",
    "        lemmatised_sentence_dict[lemmatised_word] = original_word\n",
    "    return(lemmatised_sentence_list, lemmatised_sentence_dict)\n",
    "\n",
    "\n",
    "# get encoding number for a specific word\n",
    "def get_word_code(word):\n",
    "    encoded_word = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "    word_code = int(encoded_word.input_ids[0][1]) # get token code for target word\n",
    "    return(word_code)\n",
    "\n",
    "\n",
    "# Function to import a word embedding model from a file\n",
    "def import_model(layer, type={'raw','normalised'}, full_import=False, vocab_set=[]):\n",
    "    \"\"\" string -> None\n",
    "    Imports an embedding model, storing it in the model_embed_storage dictionary.\n",
    "    \"\"\"\n",
    "        \n",
    "    # open relevant file\n",
    "    filename = path_base+'Combined Embeddings\\\\'+type+'_'+str(layer)+'.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    model_dict = {} # create word dictionary for specific model\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        if full_import==False and word in vocab_set: # only  words for testing if full_import==False\n",
    "            embedding_list = [float(x) for x in word_list[1:-1]] # store embeddings\n",
    "            embedding_np = np.array(embedding_list)\n",
    "            model_dict[word] = embedding_np\n",
    "        elif full_import==True: # this will import all words in the vocab set, not just those for testing\n",
    "            embedding_list = [float(x) for x in word_list[1:-1]] # store embeddings\n",
    "            embedding_np = np.array(embedding_list)\n",
    "            model_dict[word] = embedding_np\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return(model_dict)\n",
    "\n",
    "\n",
    "# Get list of verbs that have already had their sense embeddings saved\n",
    "def get_done_senses(path_to_done_senses):\n",
    "    verb_file_list = os.listdir(path=path_to_done_senses)\n",
    "    done_verbs = []\n",
    "    for verb in verb_file_list:\n",
    "        verb_list = verb.split('_')\n",
    "        done_verbs.append(verb_list[0]+'_'+verb_list[1])\n",
    "    done_verbs_set = list(set(done_verbs))\n",
    "    done_verbs_set.sort()\n",
    "    return(done_verbs_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9313526e-3a99-4241-bc16-92cbc45c59db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load embedding model and sentences dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba751b47-3b93-42d8-b944-9633d62e26b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ernie-2.0-base-en model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load transformer model\n",
    "model_name = 'ernie-2.0-base-en' # specify model to load\n",
    "ernie_path = path_base+'Sentence Embeddings//'+model_name\n",
    "logging.set_verbosity_error() # turn off annoying model initialisation warning\n",
    "config_state = AutoConfig.from_pretrained(ernie_path, output_hidden_states=True) # get hidden states\n",
    "tokenizer = AutoTokenizer.from_pretrained(ernie_path)\n",
    "model = AutoModel.from_pretrained(ernie_path, config=config_state)\n",
    "print(model_name+' model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc1533c-e5f7-496f-b6f9-7ec6bc62a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimVerb_mod vocab loaded\n",
      "822 words\n"
     ]
    }
   ],
   "source": [
    "# Load vocab set\n",
    "dataset_name = 'SimVerb_mod' # specify vocal set to load\n",
    "dataset, _ = load_sim_dataset('EN-SimVerb-3200-mod-uk')\n",
    "vocab = []\n",
    "for word_pair in dataset:\n",
    "    vocab.append(word_pair[0])\n",
    "    vocab.append(word_pair[1])\n",
    "vocab_set = list(set(vocab))\n",
    "vocab_set.sort()\n",
    "print(dataset_name+' vocab loaded')\n",
    "print(str(len(vocab_set))+' words')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "affd45c0",
   "metadata": {},
   "source": [
    "### Compute and save contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb41182-8d96-4911-9655-94b70a4b32a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m sent_embedding_raw \u001b[39m=\u001b[39m sent_model_output\u001b[39m.\u001b[39mhidden_states[layer]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m] \u001b[39m# get sentence embeddings\u001b[39;00m\n\u001b[0;32m     52\u001b[0m word_embedding \u001b[39m=\u001b[39m sent_embedding_raw[target_index]\n\u001b[1;32m---> 53\u001b[0m save_embeddings_word(verb_sense, word_embedding, layer, save_path)\n",
      "Cell \u001b[1;32mIn[2], line 88\u001b[0m, in \u001b[0;36msave_embeddings_word\u001b[1;34m(word, word_embeddings, layer, path)\u001b[0m\n\u001b[0;32m     86\u001b[0m final_string \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(word_embeddings)[\u001b[39m2\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m# don't include brackets in string\u001b[39;00m\n\u001b[0;32m     87\u001b[0m save_file\u001b[39m.\u001b[39mwritelines(final_string)\n\u001b[1;32m---> 88\u001b[0m save_file\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get contextual embeddings for each word in vocab set and save each to separate file\n",
    "irregular_verbs = {'bear':'bore', 'cope':'coping', 'depends':'depends'}\n",
    "verb_senses_corpus_folder = path_base+'Corpus Data\\Dictionary Verb Corpus\\\\'\n",
    "verb_senses_list = os.listdir(path=verb_senses_corpus_folder)\n",
    "\n",
    "for verb_sense_file in verb_senses_list:\n",
    "    if verb_sense_file.endswith(\".txt\"):\n",
    "        \n",
    "        # Get target verb and verb sense\n",
    "        target_verb = verb_sense_file[0:-6] # remove filetype and sense ID to get just the target verb\n",
    "        verb_sense = verb_sense_file[0:-4] # get the verb sense\n",
    "        \n",
    "        # Or just get the word if we aren't using sense embeddings\n",
    "        if verb_sense in get_done_senses(verb_senses_corpus_folder+'Embeddings'): # skip verbs we've already got the embeddings for\n",
    "            continue\n",
    "\n",
    "        # Open list of corpus sentences for target word\n",
    "        with open(verb_senses_corpus_folder+verb_sense_file, encoding=\"utf-8\") as file:\n",
    "            sentence_list = file.readlines()\n",
    "        \n",
    "        # Get embedding from each sentence in list\n",
    "        if len(sentence_list)>0:\n",
    "            for sentence in sentence_list:\n",
    "                # get lemmatiseed list of words in sentence\n",
    "                sentence = sentence.lower()\n",
    "                sent_encoded_input = tokenizer(sentence, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "                encoded_word_ids = np.array(sent_encoded_input.input_ids[0])\n",
    "                sent_model_output = model(**sent_encoded_input)\n",
    "                lemmatised_sentence_list, lemmatised_sentence_dict = lemmatise_sent(sentence) # get lemmatised version of the sentence\n",
    "\n",
    "                # get sense embedding from the sentence\n",
    "                try:\n",
    "                    target_verb_conj = lemmatised_sentence_dict[target_verb] # get the conjugated form of the target verb\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        target_verb_conj = irregular_verbs[target_verb] # get the conjugated form of the target verb if the verb has irregular conjugation\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                try:\n",
    "                    target_verb_conj_code = int(tokenizer(target_verb_conj, return_tensors='pt').input_ids[0][1]) # extract the token code for the target verb\n",
    "                    target_index = list(np.array(sent_encoded_input.input_ids[0])).index(target_verb_conj_code) # find the index of the target token in our sentence\n",
    "                except:\n",
    "                    print('Couldn\\'t find the word: '+target_verb_conj)\n",
    "                # print(encoded_word_ids[target_index])\n",
    "                # print(target_word_conjugated)\n",
    "                # print(lemmatised_sentence_list)\n",
    "\n",
    "                # get embeddings for each layer of network\n",
    "                for layer in range(1,13):\n",
    "                    save_path = verb_senses_corpus_folder+'Embeddings\\\\'\n",
    "                    sent_embedding_raw = sent_model_output.hidden_states[layer].detach().numpy()[0] # get sentence embeddings\n",
    "                    word_embedding = sent_embedding_raw[target_index]\n",
    "                    save_embeddings_word(verb_sense, word_embedding, layer, save_path) # save embeddings to file by layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "654609b0-c2e2-4cb3-b0a4-aff01ea3166d",
   "metadata": {},
   "source": [
    "### Put together full set of embeddings into single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f725a4d1-1191-4e31-af4f-30d847a2423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dictionary of all contextualised word embeddings\n",
    "file_list = os.listdir(path=verb_senses_corpus_folder+'Embeddings')\n",
    "word_embed_dict = {}\n",
    "for layer in range(1,13):\n",
    "    layer_dict = {}\n",
    "    for file in file_list: # read all words that we have sense embeddings for\n",
    "        file_base = file.split('.')[0] # remove file type\n",
    "        sense = file_base[0:-2].strip('_') # remove layer\n",
    "        filename = verb_senses_corpus_folder+'Embeddings\\\\'+sense+'_'+str(layer)+'.txt' # get the file for words\n",
    "        try:\n",
    "            with open(filename) as file:\n",
    "                np_lines = np.loadtxt(file)\n",
    "                layer_dict[sense] = np_lines\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    word_embed_dict[layer] = layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f6d1513-1cde-4328-8e80-02c204f68922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw contextualised embeddings to a single text file per ERNIE layer\n",
    "for layer in range(1,13):\n",
    "    for word in word_embed_dict[layer].keys():\n",
    "        if len(np.ndarray.flatten(word_embed_dict[layer][word])) > 800: # only if we only have multiple embeddings\n",
    "            contextual_embedding = np.mean(word_embed_dict[layer][word], axis=0) # average over all saved embeddings\n",
    "        else:\n",
    "            contextual_embedding = word_embed_dict[layer][word]\n",
    "        save_path = 'raw_ernie_dict_embed_'+str(layer)+'.txt'\n",
    "\n",
    "        with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "            final_string = word+' '+str(contextual_embedding)[2:-1] # don't include brackets in string\n",
    "            save_file.writelines(final_string)\n",
    "            save_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9684558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save normalised transformer embeddings to a single text file per ERNIE layer\n",
    "for layer in range(1,13):\n",
    "\n",
    "    # Open file with unnormalised embeddings\n",
    "    filename = 'raw_ernie_dict_embed_'+str(layer)+'.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    # Load values into dictionary\n",
    "    model_dict = {}\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        embedding_list = [float(x) for x in word_list[1:-1]]\n",
    "        embedding_np = np.array(embedding_list)\n",
    "        model_dict[word] = embedding_np\n",
    "\n",
    "    # Convert to numpy array\n",
    "    first_key = list(model_dict.keys())[0]\n",
    "    length = len(model_dict[first_key])\n",
    "    model_np = np.empty((0,length), float)\n",
    "    for word in model_dict.keys():\n",
    "        model_np = np.vstack([model_np, model_dict[word]])\n",
    "\n",
    "    # Normalise array\n",
    "    mean_np = np.mean(model_np,axis=0)\n",
    "    std_np = np.std(model_np, axis=1)\n",
    "    mean_tp_np = np.transpose(model_np - mean_np)\n",
    "    model_final_np = np.transpose(mean_tp_np/std_np)\n",
    "\n",
    "    # Save normalised embeddings to new file\n",
    "    save_path = 'normalised_ernie_dict_embed_'+str(layer)+'.txt'\n",
    "    i=0\n",
    "    with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "        for word in model_dict.keys():\n",
    "            final_string = word+' '+str(model_final_np[i,:])[1:-1] # remove brackets from numpy\n",
    "            save_file.writelines(final_string)\n",
    "            save_file.write('\\n')\n",
    "            i=i+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9020756",
   "metadata": {},
   "source": [
    "### Combine senses together (senses to words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63034771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the senses into a dictionary for each word\n",
    "file_location = path_base+'Sense Embeddings\\Dictionary Sense Embeddings\\Embeddings\\\\'\n",
    "verb_senses_list = os.listdir(file_location)\n",
    "sentence_storage = {}\n",
    "for verb_sense in verb_senses_list:\n",
    "    with open(file_location+verb_sense, encoding=\"utf-8\") as file:\n",
    "        sentence_list = file.readlines()\n",
    "    verb = verb_sense[0:-6].strip('_')\n",
    "    try:\n",
    "        sentence_storage[verb] = sentence_storage[verb] + sentence_list\n",
    "    except KeyError:\n",
    "        sentence_storage[verb] = sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad88193e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abduct_1', 'abstain_1', 'abuse_1', 'abuse_2', 'accelerate_1', 'accept_1', 'accept_2', 'accomplish_1', 'accumulate_1', 'accumulate_2', 'accuse_1', 'ache_1', 'ache_2', 'achieve_1', 'achieve_2', 'acknowledge_1', 'acknowledge_2', 'acquire_1', 'acquit_1', 'address_1', 'address_2', 'address_3', 'add_1', 'add_2', 'adjourn_1', 'admire_1', 'admit_1', 'admit_2', 'adopt_1', 'adopt_2', 'adopt_3', 'adore_1', 'adorn_1', 'advance_1', 'advance_2', 'advance_3', 'advise_1', 'affect_1', 'affect_2', 'aggravate_1', 'aggravate_2', 'agree_1', 'agree_2', 'aid_1', 'aim_1', 'aim_2', 'allow_1', 'allow_2', 'alter_1', 'amaze_1', 'amuse_1', 'analyze_1', 'angle_1', 'angle_2', 'annihilate_1', 'announce_1', 'annoy_1', 'appear_1', 'appear_2', 'appear_3', 'apply_1', 'apply_2', 'apply_3', 'approach_1', 'approach_2', 'approach_3', 'approve_1', 'approve_2', 'argue_1', 'argue_2', 'arrest_1', 'arrive_1', 'arrive_2', 'ascend_1', 'ascend_2', 'ask_1', 'ask_2', 'assault_1', 'assist_1', 'associate_1', 'associate_2', 'assume_1', 'assume_2', 'attach_1', 'attach_2', 'attack_1', 'attack_2', 'attack_3', 'attempt_1', 'attend_1', 'attend_2', 'attract_1', 'avoid_1', 'awake_1', 'award_1', 'bake_1', 'ban_1', 'bargain_1', 'barter_1', 'bear_1', 'bear_2', 'bear_3', 'bear_4', 'beat_1', 'beat_2', 'beat_3', 'beat_4', 'beat_5', 'become_1', 'become_2', 'begin_1', 'beg_1', 'believe_1', 'belong_1', 'bend_1', 'bend_2', 'bend_3', 'betray_1', 'betray_2', 'bet_1', 'beware_1', 'bias_1', 'bite_1', 'bite_2', 'blame_1', 'bleach_1', 'blend_1', 'bless_1', 'block_1', 'blow_1', 'blow_2', 'blow_3', 'bluff_1', 'blur_1', 'blush_1', 'boil_1', 'boom_1', 'boom_2', 'boost_1', 'borrow_1', 'bother_1', 'bother_2', 'bounce_1', 'bounce_2', 'bound_1', 'bow_1', 'break_1', 'break_2', 'break_3', 'break_4', 'break_5', 'break_6', 'breathe_1', 'breed_1', 'breed_2', 'bring_1', 'bring_2', 'broil_1', 'brush_1', 'brush_2', 'build_1', 'bumble_1', 'bump_1', 'bump_2', 'burn_1', 'burn_2', 'burn_3', 'burst_1', 'burst_2', 'burst_3', 'bury_1', 'buy_1', 'buy_2', 'buzz_1', 'buzz_2', 'buzz_3', 'calculate_1', 'call_1', 'call_2', 'call_3', 'call_4', 'call_5', 'call_6', 'capitulate_1', 'capture_1', 'capture_2', 'care_1', 'carry_1', 'carry_2', 'carry_3', 'carry_4', 'carry_5', 'carry_6', 'cast_1', 'cast_2', 'cast_3', 'cast_4', 'cast_5', 'cast_6', 'catch_1', 'catch_2', 'catch_3', 'catch_4', 'catch_5', 'catch_6', 'celebrate_1', 'certify_1', 'change_1', 'change_2', 'change_3', 'charge_1', 'charge_2', 'charge_3', 'charge_4', 'charge_5', 'chase_1', 'cheat_1', 'chew_1', 'chirp_1', 'choke_1', 'choke_2', 'choke_3', 'choose_1', 'chop_1', 'chuck_1', 'chuck_2', 'claim_1', 'claim_2', 'clarify_1', 'clench_1', 'climb_1', 'climb_2', 'climb_3', 'collaborate_1', 'collect_1', 'collect_2', 'color_1', 'color_2', 'combine_1', 'combine_2', 'come_1', 'come_2', 'come_3', 'comfort_1', 'command_1', 'command_2', 'command_3', 'commit_1', 'commit_2', 'commit_3', 'communicate_1', 'communicate_2', 'compete_1', 'complain_1', 'compose_1', 'compose_2', 'compose_3', 'compound_1', 'compound_2', 'comprehend_1', 'compute_1', 'concentrate_1', 'concentrate_2', 'concern_1', 'concern_2', 'concur_1', 'condemn_1', 'condemn_2', 'condemn_3', 'condemn_4', 'confess_1', 'confuse_1', 'confuse_2', 'connect_1', 'connect_2', 'connect_3', 'conquer_1', 'console_1', 'conspire_1', 'construct_1', 'control_1', 'control_2', 'convince_1', 'cook_1', 'cooperate_1', 'cope_1', 'correct_1', 'corrupt_1', 'cough_1', 'count_1', 'count_2', 'count_3', 'cover_1', 'cover_2', 'cover_3', 'cover_4', 'cover_5', 'crackle_1', 'crack_1', 'crack_2', 'crack_3', 'crack_4', 'crash_1', 'crash_2', 'crash_3', 'crave_1', 'crawl_1', 'crawl_2', 'create_1', 'criticize_1', 'croak_1', 'crochet_1', 'cruise_1', 'crunch_1', 'crunch_2', 'crush_1', 'crush_2', 'crush_3', 'cry_1', 'cry_2', 'cuddle_1', 'damn_1', 'damn_2', 'dance_1', 'dare_1', 'dare_2', 'dash_1', 'dash_2', 'deal_1', 'deal_2', 'debate_1', 'decay_1', 'deceive_1', 'decide_1', 'decide_2', 'decline_1', 'decline_2', 'decompose_1', 'decompose_2', 'decorate_1', 'decorate_2', 'decrease_1', 'dedicate_1', 'dedicate_2', 'defeat_1', 'defend_1', 'defend_2', 'define_1', 'defrost_1', 'degrade_1', 'degrade_2', 'delay_1', 'deliver_1', 'deliver_2', 'deliver_3', 'deliver_4', 'deliver_5', 'demand_1', 'demand_2', 'demolish_1', 'deny_1', 'deny_2', 'depart_1', 'depends_1', 'depend_1', 'deposit_1', 'descend_1', 'descend_2', 'describe_1', 'design_1', 'desire_1', 'despair_1', 'despise_1', 'destroy_1', 'detach_1', 'deteriorate_1', 'determine_1', 'determine_2', 'determine_3', 'develop_1', 'develop_2', 'develop_3', 'devote_1', 'die_1', 'differ_1', 'differ_2', 'dig_1', 'dig_2', 'diminish_1', 'dip_1', 'dip_2', 'direct_1', 'direct_2', 'direct_3', 'disagree_1', 'disallow_1', 'disappear_1', 'disappear_2', 'disappear_3', 'disappoint_1', 'disbelieve_1', 'discharge_1', 'discharge_2', 'discharge_3', 'discharge_4', 'discourage_1', 'discourage_2', 'discover_1', 'discuss_1', 'disguise_1', 'disgust_1', 'disintegrate_1', 'dislike_1', 'dismay_1', 'dismiss_1', 'dismiss_2', 'dismiss_3', 'disown_1', 'disperse_1', 'dispose_1', 'dispose_2', 'dispose_3', 'disprove_1', 'disregard_1', 'dissolve_1', 'dissolve_2', 'disturb_1', 'disturb_2', 'disturb_3', 'dive_1', 'dive_2', 'dive_3', 'divide_1', 'divide_2', 'divide_3', 'dominate_1', 'doubt_1', 'doze_1', 'drag_1', 'drag_2', 'drag_3', 'drain_1', 'drain_2', 'draw_1', 'draw_2', 'draw_3', 'draw_4', 'draw_5', 'draw_6', 'drench_1', 'drift_1', 'drift_2', 'drink_1', 'drink_2', 'drip_1', 'drive_1', 'drive_2', 'drive_3', 'drown_1', 'drown_2', 'drown_3', 'duck_1', 'duck_2', 'dump_1', 'dump_2', 'dunk_1', 'dunk_2', 'duplicate_1', 'earn_1', 'earn_2', 'educate_1', 'elect_1', 'elect_2', 'embarrass_1', 'embrace_1', 'embrace_2', 'encourage_1', 'encourage_2', 'enforce_1', 'engage_1', 'engage_2', 'engage_3', 'engage_4', 'enjoy_1', 'enrage_1', 'ensure_1', 'entertain_1', 'entertain_2', 'enter_1', 'enter_2', 'enter_3', 'erode_1', 'erupt_1', 'erupt_2', 'escape_1', 'escape_2', 'evacuate_1', 'evaluate_1', 'evict_1', 'examine_1', 'examine_2', 'exceed_1', 'excel_1', 'exchange_1', 'excuse_1', 'excuse_2', 'excuse_3', 'exhale_1', 'exist_1', 'exist_2', 'explain_1', 'explode_1', 'explode_2', 'explode_3', 'exploit_1', 'exploit_2', 'express_1', 'face_1', 'face_2', 'fade_1', 'fail_1', 'fail_2', 'fail_3', 'fall_1', 'fall_2', 'fall_3', 'fasten_1', 'fasten_2', 'feed_1', 'feed_2', 'feel_1', 'feel_2', 'feel_3', 'fight_1', 'fight_2', 'fill_1', 'fill_2', 'fill_3', 'find_1', 'find_2', 'finish_1', 'fit_1', 'fit_2', 'fix_1', 'fix_2', 'fix_3', 'flap_1', 'flash_1', 'flash_2', 'flee_1', 'flex_1', 'fling_1', 'flip_1', 'float_1', 'float_2', 'flow_1', 'flow_2', 'flush_1', 'flush_2', 'flutter_1', 'fly_1', 'fly_2', 'foil_1', 'fold_1', 'fold_2', 'follow_1', 'follow_2', 'follow_3', 'follow_4', 'follow_5', 'forbear_1', 'forbid_1', 'forget_1', 'forget_2', 'forgive_1', 'frame_1', 'frame_2', 'frame_3', 'freeze_1', 'freeze_2', 'free_1', 'free_2', 'frighten_1', 'frustrate_1', 'frustrate_2', 'fry_1', 'gain_1', 'gamble_1', 'gather_1', 'gather_2', 'get_1', 'get_2', 'get_3', 'get_4', 'giggle_1', 'give_1', 'give_2', 'give_3', 'glance_1', 'glare_1', 'glide_1', 'glow_1', 'go_1', 'go_2', 'go_3', 'grab_1', 'grab_2', 'graduate_1', 'grant_1', 'grant_2', 'grasp_1', 'grasp_2', 'grate_1', 'grate_2', 'graze_1', 'graze_2', 'greet_1', 'greet_2', 'grill_1', 'grill_2', 'grind_1', 'grow_1', 'grow_2', 'guess_1', 'guess_2', 'gulp_1', 'hamper_1', 'handle_1', 'handle_2', 'handle_3', 'hang_1', 'hang_2', 'happen_1', 'hatch_1', 'hate_1', 'haul_1', 'haunt_1', 'haunt_2', 'heal_1', 'hear_1', 'hear_2', 'help_1', 'hesitate_1', 'hide_1', 'hide_2', 'hide_3', 'hide_4', 'hike_1', 'hitch_1', 'hitch_2', 'hit_1', 'hit_2', 'hit_3', 'hit_4', 'hold_1', 'hold_2', 'hold_3', 'hold_4', 'hold_5', 'hold_6', 'holler_1', 'hoot_1', 'hope_1', 'hop_1', 'hop_2', 'hug_1', 'hug_2', 'humiliate_1', 'hum_1', 'hum_2', 'hunt_1', 'hunt_2', 'hurry_1', 'hurt_1', 'hurt_2', 'hustle_1', 'hypnotize_1', 'ignore_1', 'imagine_1', 'imagine_2', 'imitate_1', 'impersonate_1', 'implode_1', 'implode_2', 'impose_1', 'impose_2', 'incline_1', 'incline_2', 'include_1', 'increase_1', 'inform_1', 'inhale_1', 'inquire_1', 'inspect_1', 'instruct_1', 'instruct_2', 'insult_1', 'insure_1', 'integrate_1', 'integrate_2', 'intend_1', 'interrupt_1', 'intoxicate_1', 'introduce_1', 'introduce_2', 'invalidate_1', 'invalidate_2', 'invent_1', 'invite_1', 'invite_2', 'involve_1', 'involve_2', 'irritate_1', 'irritate_2', 'jerk_1', 'jog_1', 'join_1', 'join_2', 'join_3', 'judge_1', 'jump_1', 'jump_2', 'jump_3', 'justify_1', 'keep_1', 'keep_2', 'keep_3', 'kick_1', 'kidnap_1', 'kill_1', 'kill_2', 'kiss_1', 'kneel_1', 'knit_1', 'knit_2', 'knock_1', 'know_1', 'know_2', 'know_3', 'laugh_1', 'launch_1', 'launch_2', 'lay_1', 'lay_2', 'lead_1', 'lead_2', 'lead_3', 'lead_4', 'lead_5', 'lean_1', 'lean_2', 'leap_1', 'leap_2', 'learn_1', 'leave_1', 'leave_2', 'leave_3', 'leave_4', 'leave_5', 'lend_1', 'let_1', 'lick_1', 'lie_1', 'lie_2', 'lie_3', 'lift_1', 'lift_2', 'like_1', 'like_2', 'limit_1', 'limit_2', 'limp_1', 'listen_1', 'listen_2', 'litter_1', 'live_1', 'live_2', 'live_3', 'look_1', 'look_2', 'look_3', 'loosen_1', 'lose_1', 'lose_2', 'lose_3', 'lounge_1', 'lower_1', 'magnify_1', 'make_1', 'make_2', 'make_3', 'make_4', 'manage_1', 'manage_2', 'manage_3', 'marry_1', 'matter_1', 'mean_1', 'mean_2', 'mean_3', 'mean_4', 'meet_1', 'meet_2', 'meet_3', 'melt_1', 'melt_2', 'mend_1', 'mimic_1', 'mind_1', 'mind_2', 'misspend_1', 'miss_1', 'miss_2', 'miss_3', 'miss_4', 'miss_5', 'mistreat_1', 'mix_1', 'mix_2', 'move_1', 'move_2', 'move_3', 'move_4', 'move_5', 'mow_1', 'multiply_1', 'multiply_2', 'munch_1', 'nag_1', 'nag_2', 'nap_1', 'need_1', 'neglect_1', 'neglect_2', 'notify_1', 'nullify_1', 'obey_1', 'object_1', 'observe_1', 'observe_2', 'obtain_1', 'offend_1', 'offend_2', 'operate_1', 'operate_2', 'operate_3', 'operate_4', 'organize_1', 'organize_2', 'originate_1', 'outnumber_1', 'overcome_1', 'overcome_2', 'overflow_1', 'overpower_1', 'overwhelm_1', 'own_1', 'paint_1', 'paint_2', 'pardon_1', 'participate_1', 'pass_1', 'pass_2', 'pass_3', 'pass_4', 'pass_5', 'pass_6', 'pat_1', 'pause_1', 'pay_1', 'pay_2', 'pelt_1', 'pelt_2', 'perceive_1', 'perceive_2', 'perform_1', 'perform_2', 'perform_3', 'perish_1', 'permit_1', 'perspire_1', 'persuade_1', 'pick_1', 'pick_2', 'pinch_1', 'pinch_2', 'play_1', 'play_2', 'play_3', 'play_4', 'play_5', 'plead_1', 'plead_2', 'please_1', 'please_2', 'plow_1', 'pluck_1', 'pluck_2', 'poach_1', 'poach_2', 'poach_3', 'poise_1', 'polish_1', 'ponder_1', 'pop_1', 'pop_2', 'portray_1', 'portray_2', 'possess_1', 'possess_2', 'pounce_1', 'pour_1', 'pour_2', 'practice_1', 'practice_2', 'practice_3', 'pray_1', 'pray_2', 'predict_1', 'prefer_1', 'prepare_1', 'prepare_2', 'press_1', 'press_2', 'press_3', 'press_4', 'presume_1', 'pretend_1', 'preview_1', 'prick_1', 'print_1', 'print_2', 'print_3', 'produce_1', 'produce_2', 'produce_3', 'produce_4', 'prohibit_1', 'promise_1', 'promise_2', 'promote_1', 'promote_2', 'promote_3', 'prosecute_1', 'prosper_1', 'protect_1', 'protest_1', 'protest_2', 'prove_1', 'prove_2', 'prove_3', 'pull_1', 'pull_2', 'punch_1', 'punch_2', 'punch_3', 'punish_1', 'pup_1', 'purchase_1', 'push_1', 'push_2', 'push_3', 'push_4', 'put_1', 'put_2', 'put_3', 'put_4', 'quench_1', 'quench_2', 'raise_1', 'raise_2', 'raise_3', 'raise_4', 'raise_5', 'raise_6', 'rap_1', 'rap_2', 'rationalize_1', 'rationalize_2', 'rattle_1', 'rattle_2', 'reach_1', 'reach_2', 'reach_3', 'reach_4', 'reach_5', 'read_1', 'read_2', 'read_3', 'realize_1', 'realize_2', 'reappear_1', 'reap_1', 'rearrange_1', 'rebel_1', 'recall_1', 'recall_2', 'receive_1', 'receive_2', 'receive_3', 'recommend_1', 'recruit_1', 'recycle_1', 'reduce_1', 'reel_1', 'reflect_1', 'reflect_2', 'reflect_3', 'refrain_1', 'refuse_1', 'refuse_2', 'register_1', 'register_2', 'register_3', 'regret_1', 'reject_1', 'rejoice_1', 'relax_1', 'relax_2', 'release_1', 'release_2', 'release_3', 'rely_1', 'rely_2', 'remain_1', 'remain_2', 'remain_3', 'remark_1', 'remember_1', 'remember_2', 'remind_1', 'remove_1', 'remove_2', 'remove_3', 'renounce_1', 'repair_1', 'repeat_1', 'repeat_2', 'replace_1', 'replace_2', 'reply_1', 'repress_1', 'repress_2', 'reprimand_1', 'reproduce_1', 'reproduce_2', 'request_1', 'rescue_1', 'reserve_1', 'respect_1', 'respect_2', 'respond_1', 'respond_2', 'restore_1', 'restrict_1', 'result_1', 'retain_1', 'retire_1', 'retire_2', 'retire_3', 'retreat_1', 'retreat_2', 'return_1', 'return_2', 'return_3', 'return_4', 'review_1', 'review_2', 'ride_1', 'ride_2', 'ride_3', 'ridicule_1', 'rinse_1', 'rinse_2', 'rinse_3', 'rip_1', 'rise_1', 'rise_2', 'rise_3', 'rise_4', 'risk_1', 'roam_1', 'roar_1', 'rob_1', 'rub_1', 'rub_2', 'rub_3', 'rub_4', 'run_1', 'run_2', 'run_3', 'run_4', 'run_5', 'run_6', 'rush_1', 'rush_2', 'salute_1', 'save_1', 'save_2', 'save_3', 'say_1', 'say_2', 'scare_1', 'scold_1', 'scramble_1', 'scramble_2', 'scratch_1', 'scratch_2', 'scream_1', 'scribble_1', 'search_1', 'secure_1', 'secure_2', 'secure_3', 'seek_1', 'seek_2', 'seem_1', 'see_1', 'see_2', 'see_3', 'see_4', 'see_5', 'see_6', 'seize_1', 'seize_2', 'seize_3', 'select_1', 'sell_1', 'sell_2', 'send_1', 'send_2', 'separate_1', 'separate_2', 'serve_1', 'serve_2', 'serve_3', 'serve_4', 'serve_5', 'settle_1', 'settle_2', 'settle_3', 'settle_4', 'settle_5', 'set_1', 'set_3', 'set_4', 'set_5', 'set_6', 'set_7', 'sew_1', 'shake_1', 'shake_2', 'shatter_1', 'shatter_2', 'shift_1', 'shift_2', 'shine_1', 'shine_2', 'shine_3', 'shine_4', 'shiver_1', 'shoot_1', 'shoot_2', 'shoot_3', 'shoot_5', 'shove_1', 'show_1', 'show_2', 'show_3', 'show_4', 'show_5', 'show_6', 'shrink_1', 'shrink_2', 'shrink_3', 'shuffle_1', 'shuffle_2', 'shun_1', 'shut_1', 'shut_2', 'sing_1', 'sink_1', 'sink_2', 'sink_3', 'sink_5', 'sip_1', 'sit_1', 'sit_2', 'sit_3', 'sit_4', 'skip_1', 'skip_2', 'slap_1', 'slay_1', 'slice_1', 'slide_1', 'slide_2', 'slip_1', 'slip_2', 'slip_3', 'slip_5', 'slither_1', 'slurp_1', 'smash_1', 'smash_2', 'smear_1', 'smear_2', 'smell_1', 'smoke_1', 'smoke_2', 'smother_1', 'smother_2', 'smudge_1', 'snap_1', 'snap_2', 'snap_3', 'snatch_1', 'sneak_1', 'sneak_2', 'sneeze_1', 'sniff_1', 'snooze_1', 'snore_1', 'snuggle_1', 'soak_1', 'soar_1', 'soar_2', 'sob_1', 'solve_1', 'soothe_1', 'sow_1', 'spare_1', 'spare_2', 'spawn_1', 'speak_1', 'speak_2', 'spell_1', 'spell_2', 'spend_1', 'spend_2', 'spend_3', 'spill_1', 'spill_2', 'spin_1', 'spin_2', 'splash_1', 'split_1', 'split_2', 'spoil_1', 'spoil_2', 'sprain_1', 'spray_1', 'spread_1', 'spread_2', 'spread_3', 'spring_1', 'sprinkle_1', 'squeak_1', 'squeal_1', 'squeeze_1', 'squeeze_2', 'squeeze_3', 'squint_1', 'stab_1', 'stain_1', 'stamp_1', 'stamp_2', 'stamp_3', 'stand_1', 'stand_2', 'stand_3', 'stand_4', 'stare_1', 'start_1', 'starve_1', 'stay_1', 'stay_2', 'steal_1', 'steer_1', 'steer_2', 'stew_1', 'sting_1', 'sting_2', 'stink_1', 'stink_2', 'stir_1', 'stir_2', 'stir_3', 'stop_1', 'stop_2', 'strain_1', 'strain_2', 'strain_3', 'strain_4', 'stray_1', 'stray_2', 'stretch_1', 'stretch_2', 'stretch_3', 'strike_1', 'strike_3', 'strike_4', 'strike_5', 'strip_1', 'strip_2', 'strive_1', 'struggle_1', 'struggle_2', 'study_1', 'study_2', 'stumble_1', 'stumble_2', 'submit_1', 'subtract_1', 'succeed_1', 'succeed_2', 'suck_1', 'suck_2', 'sue_1', 'suggest_1', 'suggest_2', 'supply_1', 'suppose_1', 'suppose_2', 'surpass_1', 'surprise_1', 'survive_1', 'suspect_1', 'swallow_1', 'swallow_2', 'swap_1', 'sway_1', 'sway_2', 'swear_1', 'swear_2', 'sweat_1', 'swim_1', 'swing_1', 'swing_2', 'swing_3', 'swing_4', 'tackle_1', 'tackle_2', 'take_1', 'take_2', 'take_3', 'take_4', 'take_5', 'take_6', 'talk_1', 'tan_1', 'tap_1', 'tap_2', 'tarnish_1', 'tarnish_2', 'taste_1', 'teach_1', 'tear_1', 'tear_2', 'tear_3', 'tease_1', 'tell_1', 'tell_2', 'tell_3', 'tend_1', 'tend_2', 'testify_1', 'thaw_1', 'thaw_2', 'think_1', 'think_2', 'throw_1', 'throw_2', 'throw_3', 'tickle_1', 'tickle_2', 'tie_1', 'tie_2', 'tie_3', 'tire_1', 'toast_1', 'toast_2', 'toil_1', 'toss_1', 'toss_2', 'tote_1', 'touch_1', 'touch_2', 'touch_3', 'tow_1', 'trail_1', 'trail_2', 'trail_3', 'transplant_1', 'travel_1', 'tread_1', 'treat_1', 'treat_2', 'treat_3', 'trim_1', 'trim_2', 'trim_3', 'triumph_1', 'try_1', 'try_2', 'tug_1', 'tumble_1', 'tumble_2', 'turn_1', 'turn_2', 'understand_1', 'understand_2', 'understand_3', 'unite_1', 'unload_1', 'unload_2', 'update_1', 'upset_1', 'upset_2', 'urge_1', 'use_1', 'use_2', 'usurp_1', 'vacate_1', 'vanish_1', 'vanish_2', 'vary_1', 'veer_1', 'verify_1', 'visit_1', 'vomit_1', 'wag_1', 'wait_1', 'wake_1', 'walk_1', 'wander_1', 'wander_2', 'want_1', 'want_2', 'warn_1', 'wash_1', 'wash_2', 'waste_1', 'watch_1', 'watch_2', 'wear_1', 'wear_2', 'weave_1', 'weave_2', 'weave_3', 'wed_1', 'weep_1', 'whip_1', 'whip_2', 'whip_3', 'whisper_1', 'whistle_1', 'wiggle_1', 'win_1', 'wipe_1', 'wipe_2', 'withdraw_1', 'withdraw_2', 'withdraw_3', 'work_1', 'work_2', 'work_3', 'work_4', 'worry_1', 'worry_2', 'wrap_1', 'wreck_1', 'write_1', 'write_2', 'yawn_1', 'yearn_1', 'yell_1', 'yield_1', 'yield_2'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_storage.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddf356a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentences to a single file for each word\n",
    "for word in sentence_storage.keys():\n",
    "    with open(word+'.txt', \"a\", encoding='utf-8') as save_file:\n",
    "        for sentence in sentence_storage[word]:\n",
    "            save_file.writelines(sentence.strip('\\n'))\n",
    "            save_file.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b04072a-ffb7-4675-954d-28de669eeb79",
   "metadata": {},
   "source": [
    "### Other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4f09f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of word pairs with available senses\n",
    "\n",
    "# load list of verbs with senses available\n",
    "done_verbs = []\n",
    "for verb_sense in verb_senses_list:\n",
    "    verb_sense = verb_sense.split('.')[0] # remove tile extension\n",
    "    done_verbs.append(verb_sense[0:-2].strip('_'))\n",
    "done_verbs = list(set(done_verbs))\n",
    "\n",
    "# store verb pairs with both words available\n",
    "simverb = load_sim_dataset('EN-SimVerb-3200-mod')\n",
    "verb_dataset = []\n",
    "for word_pair in simverb[0]:\n",
    "    if word_pair[0] in done_verbs and word_pair[1] in done_verbs:\n",
    "        verb_dataset.append(word_pair)\n",
    "        \n",
    "# Save a list of the words that have been used for further analysis\n",
    "save_file = open(path_base+'\\\\vocab_mini.txt', \"a\", encoding='utf-8')\n",
    "i=0\n",
    "for line in verb_dataset:\n",
    "    save_file.writelines(line[0]+'\\t'+line[1]+'\\t'+line[2])\n",
    "    save_file.write('\\n')\n",
    "    i=i+1\n",
    "save_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd19f865deb76414f800cd5e170dbd2fd2287196326fe3642087e61bd5d12a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
