{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f9c5eed-3731-4a0b-8ce1-6abcbc22ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for calculating Dictionary Sense Embeddings using ERNIE transformer\n",
    "## James Fodor 2022\n",
    "## Python 3.8\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import logging\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Set print option for numpy, needed for saving embeddings\n",
    "np.set_printoptions(precision=4, threshold=10000, linewidth=100000, suppress=True, floatmode='fixed')\n",
    "\n",
    "# Define base path location for data\n",
    "path_base = 'D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\\\'\n",
    "\n",
    "# Get wordnet to work\n",
    "from nltk.data import path # need to specify the location of the nltk data\n",
    "path.append(path_base+\"\\Frames and Structured Data\\FrameNet\\\\nltk_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "271b3932-1165-40ee-ae40-5c6988bcef9c",
   "metadata": {},
   "source": [
    "### Functions for getting sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af5e65a-7b98-4b28-85ac-a3ae33dc3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_sim(embed_1, embed_2):\n",
    "    \"\"\" numpy_array, numpy_array -> float\n",
    "    Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "    \"\"\"\n",
    "    if np.dot(embed_1,embed_2) == 0:\n",
    "        similarity = 0 # don't normalise if similarity is zero\n",
    "    else:\n",
    "        similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "    return(similarity)\n",
    "\n",
    "\n",
    "# Get decontextualised transformer embedding for given word\n",
    "def transformer_embed_decontext(model, tokenizer, word, layer=0, embed_type='decontext'):\n",
    "    \"\"\" pytorch_model, pytorch_tokenzier, str, int, str -> np_array\n",
    "    Extracts a word embedding given a pythorch model and tokenizer. Three modes of operation\n",
    "    depending on how to get the word embedding.\n",
    "    \"\"\"\n",
    "    encoded_input = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "    model_output = model(**encoded_input)\n",
    "    word_embedding_raw = model_output.hidden_states[layer].detach().numpy()[0]\n",
    "     \n",
    "    if embed_type=='mean': # take the mean of all tokens\n",
    "        word_embedding = word_embedding_raw.mean(axis=0)\n",
    "    elif embed_type=='cls': # use the 'CLS' token\n",
    "        word_embedding = word_embedding_raw[0]\n",
    "    elif embed_type=='decontext': # take the mean of word tokens only\n",
    "        word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "\n",
    "    return(word_embedding)\n",
    "\n",
    "\n",
    "# Load word similarity dataset\n",
    "def load_sim_dataset(dataset):\n",
    "    \"\"\" str -> (list_str, np_array)\n",
    "    Loads a dataset of word similarities, returning the word pairs and similarity ratings.\n",
    "    \"\"\"\n",
    "    path = path_base+'Word Similarity Data\\Word Similarities Final\\\\'\n",
    "    filename = path+dataset+'.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    wordpairs = [None]*len(lines) # initialise storage\n",
    "    ratings = [None]*len(lines)\n",
    "    i=0\n",
    "    for line in lines:\n",
    "        line = line.strip() # remove new line chars\n",
    "        wordpairs[i] = line.split() # split at any whitespace chars\n",
    "        ratings[i] = float(wordpairs[i][2])\n",
    "        i=i+1\n",
    "    ratings = np.array(ratings)\n",
    "\n",
    "    return(wordpairs, ratings)\n",
    "\n",
    "\n",
    "# Save contextual embeddings for a given word to a new file for each word\n",
    "def save_embeddings_word(word, word_embeddings, layer, path):\n",
    "    \"\"\" str, np_array, int, str -> None\n",
    "    Saves word_embeddings for specified word to a specified path.\n",
    "    \"\"\"\n",
    "    save_path = path+word+'_'+str(layer)+'.txt'\n",
    "    with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "        final_string = str(word_embeddings)[2:-1] # don't include brackets in string\n",
    "        save_file.writelines(final_string)\n",
    "        save_file.write('\\n')\n",
    "\n",
    "\n",
    "# Function to return a lemmatised list and dictionary for a given sentence\n",
    "def lemmatise_sent(sentence):\n",
    "    \"\"\" str -> list_str, dict\n",
    "    Takes in a sentence and returns a tokenised and lemmatised list and dictionary\n",
    "    for all words in the sentence.\n",
    "    \"\"\"\n",
    "    lemmatised_sentence_list = []\n",
    "    lemmatised_sentence_dict = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for original_word in sentence.split():\n",
    "        fixed_original_word = re.sub('[‘`’\\\"\\',;.?!\\)\\(]', '', original_word) # remove punctuation\n",
    "        lemmatised_word = lemmatizer.lemmatize(fixed_original_word, wordnet.VERB) # lematise all words in sentence\n",
    "        lemmatised_sentence_list.append(lemmatised_word)\n",
    "        lemmatised_sentence_dict[lemmatised_word] = original_word\n",
    "    return(lemmatised_sentence_list, lemmatised_sentence_dict)\n",
    "\n",
    "\n",
    "# get encoding number for a specific word\n",
    "def get_word_code(word):\n",
    "    encoded_word = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "    word_code = int(encoded_word.input_ids[0][1]) # get token code for target word\n",
    "    return(word_code)\n",
    "\n",
    "\n",
    "# Function to import a set of word embeddings from a file\n",
    "def import_model(layer, type={'raw','normalised'}, full_import=False, vocab_set=[]):\n",
    "    \"\"\" int, string, boolean, list_str -> dict\n",
    "    Imports a given layer of an embedding model, storing it in the model_embed_storage dictionary.\n",
    "    The variable vocab_set specifies the words to load if full_import is false.\n",
    "    \"\"\"\n",
    "    # open relevant file\n",
    "    filename = path_base+'Combined Embeddings\\\\'+type+'_'+str(layer)+'.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    # loop over file and store embeddings in a dictionary\n",
    "    model_dict = {} # create word dictionary for specific model\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        if full_import==False and word in vocab_set: # only words used for testing if full_import==False\n",
    "            embedding_list = [float(x) for x in word_list[1:-1]] # store embeddings\n",
    "            embedding_np = np.array(embedding_list)\n",
    "            model_dict[word] = embedding_np\n",
    "        elif full_import==True: # this will import all words in the vocab set, not just those for testing\n",
    "            embedding_list = [float(x) for x in word_list[1:-1]] # store embeddings\n",
    "            embedding_np = np.array(embedding_list)\n",
    "            model_dict[word] = embedding_np\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return(model_dict)\n",
    "\n",
    "\n",
    "# Get list of verbs that have already had their sense embeddings saved\n",
    "def get_done_senses(path_to_done_senses):\n",
    "    verb_file_list = os.listdir(path=path_to_done_senses)\n",
    "    done_verbs = []\n",
    "    for verb in verb_file_list:\n",
    "        verb_list = verb.split('_')\n",
    "        done_verbs.append(verb_list[0]+'_'+verb_list[1])\n",
    "    done_verbs_set = list(set(done_verbs))\n",
    "    done_verbs_set.sort()\n",
    "    return(done_verbs_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9313526e-3a99-4241-bc16-92cbc45c59db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load embedding model and sentences dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba751b47-3b93-42d8-b944-9633d62e26b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ernie-2.0-base-en model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load transformer model\n",
    "model_name = 'ernie-2.0-base-en' # specify model to load\n",
    "ernie_path = path_base+'Sentence Embeddings//'+model_name\n",
    "logging.set_verbosity_error() # turn off annoying model initialisation warning\n",
    "config_state = AutoConfig.from_pretrained(ernie_path, output_hidden_states=True) # get hidden states\n",
    "tokenizer = AutoTokenizer.from_pretrained(ernie_path)\n",
    "model = AutoModel.from_pretrained(ernie_path, config=config_state)\n",
    "print(model_name+' model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc1533c-e5f7-496f-b6f9-7ec6bc62a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimVerb_mod vocab loaded\n",
      "822 words\n"
     ]
    }
   ],
   "source": [
    "# Load vocab set\n",
    "dataset_name = 'SimVerb_mod' # specify vocal set to load\n",
    "dataset, _ = load_sim_dataset('EN-SimVerb-3200-mod-uk')\n",
    "vocab = []\n",
    "for word_pair in dataset:\n",
    "    vocab.append(word_pair[0])\n",
    "    vocab.append(word_pair[1])\n",
    "vocab_set = list(set(vocab))\n",
    "vocab_set.sort()\n",
    "print(dataset_name+' vocab loaded')\n",
    "print(str(len(vocab_set))+' words')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "affd45c0",
   "metadata": {},
   "source": [
    "### Compute and save contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb41182-8d96-4911-9655-94b70a4b32a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get contextual embeddings for each word in vocab set and save each to separate file\n",
    "irregular_verbs = {'bear':'bore', 'cope':'coping', 'depends':'depends'} # default lemmatiser doesn't work for these\n",
    "verb_senses_corpus_folder = path_base+'Corpus Data\\Dictionary Verb Corpus\\\\'\n",
    "verb_senses_list = os.listdir(path=verb_senses_corpus_folder)\n",
    "\n",
    "for verb_sense_file in verb_senses_list:\n",
    "    if verb_sense_file.endswith(\".txt\"):\n",
    "        \n",
    "        # Get target verb and verb sense\n",
    "        target_verb = verb_sense_file[0:-6] # remove filetype and sense ID to get just the target verb\n",
    "        verb_sense = verb_sense_file[0:-4] # get the verb sense\n",
    "        \n",
    "        # Or just get the word if we aren't using sense embeddings\n",
    "        if verb_sense in get_done_senses(verb_senses_corpus_folder+'Embeddings'): # skip verbs we've already got the embeddings for\n",
    "            continue\n",
    "\n",
    "        # Open list of corpus sentences for target word\n",
    "        with open(verb_senses_corpus_folder+verb_sense_file, encoding=\"utf-8\") as file:\n",
    "            sentence_list = file.readlines()\n",
    "        \n",
    "        # Get embedding from each sentence in list\n",
    "        if len(sentence_list)>0:\n",
    "            for sentence in sentence_list:\n",
    "                # get lemmatiseed list of words in sentence\n",
    "                sentence = sentence.lower()\n",
    "                sent_encoded_input = tokenizer(sentence, return_tensors='pt') # note use of sentence not lemmatised sentence\n",
    "                encoded_word_ids = np.array(sent_encoded_input.input_ids[0])\n",
    "                sent_model_output = model(**sent_encoded_input)\n",
    "                lemmatised_sentence_list, lemmatised_sentence_dict = lemmatise_sent(sentence) # get lemmatised version of the sentence\n",
    "\n",
    "                # get sense embedding from the sentence\n",
    "                try:\n",
    "                    target_verb_conj = lemmatised_sentence_dict[target_verb] # get the conjugated form of the target verb\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        target_verb_conj = irregular_verbs[target_verb] # get the conjugated form of the target verb if the verb has irregular conjugation\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                try:\n",
    "                    target_verb_conj_code = int(tokenizer(target_verb_conj, return_tensors='pt').input_ids[0][1]) # extract the token code for the target verb\n",
    "                    target_index = list(np.array(sent_encoded_input.input_ids[0])).index(target_verb_conj_code) # find the index of the target token in our sentence\n",
    "                except:\n",
    "                    print('Couldn\\'t find the word: '+target_verb_conj)\n",
    "                # print(encoded_word_ids[target_index])\n",
    "                # print(target_word_conjugated)\n",
    "                # print(lemmatised_sentence_list)\n",
    "\n",
    "                # get embeddings for each layer of network\n",
    "                for layer in range(1,13):\n",
    "                    save_path = verb_senses_corpus_folder+'Embeddings\\\\'\n",
    "                    sent_embedding_raw = sent_model_output.hidden_states[layer].detach().numpy()[0] # get sentence embeddings\n",
    "                    word_embedding = sent_embedding_raw[target_index]\n",
    "                    save_embeddings_word(verb_sense, word_embedding, layer, save_path) # save embeddings to file by layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "654609b0-c2e2-4cb3-b0a4-aff01ea3166d",
   "metadata": {},
   "source": [
    "### Put together full set of embeddings into single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f725a4d1-1191-4e31-af4f-30d847a2423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dictionary of all contextualised word embeddings\n",
    "file_list = os.listdir(path=verb_senses_corpus_folder+'Embeddings')\n",
    "word_embed_dict = {}\n",
    "for layer in range(1,13):\n",
    "    layer_dict = {}\n",
    "    for file in file_list: # read all words that we have sense embeddings for\n",
    "        file_base = file.split('.')[0] # remove file type\n",
    "        sense = file_base[0:-2].strip('_') # remove layer\n",
    "        filename = verb_senses_corpus_folder+'Embeddings\\\\'+sense+'_'+str(layer)+'.txt' # get the file for words\n",
    "        try:\n",
    "            with open(filename) as file:\n",
    "                np_lines = np.loadtxt(file)\n",
    "                layer_dict[sense] = np_lines\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    word_embed_dict[layer] = layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f6d1513-1cde-4328-8e80-02c204f68922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw contextualised embeddings to a single text file per ERNIE layer\n",
    "for layer in range(1,13):\n",
    "    for word in word_embed_dict[layer].keys():\n",
    "        if len(np.ndarray.flatten(word_embed_dict[layer][word])) > 800: # only if we only have multiple embeddings\n",
    "            contextual_embedding = np.mean(word_embed_dict[layer][word], axis=0) # average over all saved embeddings\n",
    "        else:\n",
    "            contextual_embedding = word_embed_dict[layer][word]\n",
    "        save_path = 'raw_ernie_dict_embed_'+str(layer)+'.txt'\n",
    "\n",
    "        with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "            final_string = word+' '+str(contextual_embedding)[2:-1] # don't include brackets in string\n",
    "            save_file.writelines(final_string)\n",
    "            save_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9684558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save normalised transformer embeddings to a single text file per ERNIE layer\n",
    "for layer in range(1,13):\n",
    "\n",
    "    # Open file with unnormalised embeddings\n",
    "    filename = 'raw_ernie_dict_embed_'+str(layer)+'.txt'\n",
    "    with open(filename) as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    # Load values into dictionary\n",
    "    model_dict = {}\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        embedding_list = [float(x) for x in word_list[1:-1]]\n",
    "        embedding_np = np.array(embedding_list)\n",
    "        model_dict[word] = embedding_np\n",
    "\n",
    "    # Convert to numpy array\n",
    "    first_key = list(model_dict.keys())[0]\n",
    "    length = len(model_dict[first_key])\n",
    "    model_np = np.empty((0,length), float)\n",
    "    for word in model_dict.keys():\n",
    "        model_np = np.vstack([model_np, model_dict[word]])\n",
    "\n",
    "    # Normalise array\n",
    "    mean_np = np.mean(model_np,axis=0)\n",
    "    std_np = np.std(model_np, axis=1)\n",
    "    mean_tp_np = np.transpose(model_np - mean_np)\n",
    "    model_final_np = np.transpose(mean_tp_np/std_np)\n",
    "\n",
    "    # Save normalised embeddings to new file\n",
    "    save_path = 'normalised_ernie_dict_embed_'+str(layer)+'.txt'\n",
    "    i=0\n",
    "    with open(save_path, \"a\", encoding='utf-8') as save_file:\n",
    "        for word in model_dict.keys():\n",
    "            final_string = word+' '+str(model_final_np[i,:])[1:-1] # remove brackets from numpy\n",
    "            save_file.writelines(final_string)\n",
    "            save_file.write('\\n')\n",
    "            i=i+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9020756",
   "metadata": {},
   "source": [
    "### Combine senses together (senses to words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63034771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the senses into a dictionary for each word\n",
    "file_location = path_base+'Sense Embeddings\\Dictionary Sense Embeddings\\Embeddings\\\\'\n",
    "verb_senses_list = os.listdir(file_location)\n",
    "sentence_storage = {}\n",
    "for verb_sense in verb_senses_list:\n",
    "    with open(file_location+verb_sense, encoding=\"utf-8\") as file:\n",
    "        sentence_list = [line.rstrip('\\n') for line in file]\n",
    "        # sentence_list = np.loadtxt(filename,  delimiter=',', dtype='str', encoding='utf-8')\n",
    "    verb = verb_sense.split('.')[0].split('_')[0].split('_')[0] # extract verb from filename\n",
    "    try:\n",
    "        sentence_storage[verb] = sentence_storage[verb] + sentence_list\n",
    "    except:\n",
    "        sentence_storage[verb] = sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf356a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentences to a single file for each word\n",
    "for word in sentence_storage.keys():\n",
    "    with open(word+'.txt', \"a\", encoding='utf-8') as save_file:\n",
    "        for sentence in sentence_storage[word]:\n",
    "            save_file.writelines(sentence.strip('\\n'))\n",
    "            save_file.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b04072a-ffb7-4675-954d-28de669eeb79",
   "metadata": {},
   "source": [
    "### Other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4f09f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of word pairs with available senses\n",
    "\n",
    "# load list of verbs with senses available\n",
    "done_verbs = []\n",
    "for verb_sense in verb_senses_list:\n",
    "    verb_sense = verb_sense.split('.')[0] # remove tile extension\n",
    "    done_verbs.append(verb_sense[0:-2].strip('_'))\n",
    "done_verbs = list(set(done_verbs))\n",
    "\n",
    "# store verb pairs with both words available\n",
    "simverb = load_sim_dataset('EN-SimVerb-3200-mod')\n",
    "verb_dataset = []\n",
    "for word_pair in simverb[0]:\n",
    "    if word_pair[0] in done_verbs and word_pair[1] in done_verbs:\n",
    "        verb_dataset.append(word_pair)\n",
    "        \n",
    "# Save a list of the words that have been used for further analysis\n",
    "save_file = open(path_base+'\\\\vocab_mini.txt', \"a\", encoding='utf-8')\n",
    "i=0\n",
    "for line in verb_dataset:\n",
    "    save_file.writelines(line[0]+'\\t'+line[1]+'\\t'+line[2])\n",
    "    save_file.write('\\n')\n",
    "    i=i+1\n",
    "save_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd19f865deb76414f800cd5e170dbd2fd2287196326fe3642087e61bd5d12a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
