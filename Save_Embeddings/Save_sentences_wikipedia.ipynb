{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to extract example sentences from Wikipedia articles\n",
    "## James Fodor 2022\n",
    "## Python 3.8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import wikipedia\n",
    "\n",
    "# File path for where vocab file is stored\n",
    "path_base = 'D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\\\'\n",
    "\n",
    "# Get wordnet to work\n",
    "from nltk.data import path # need to specify the location of the nltk data\n",
    "path.append(path_base+\"\\Frames and Structured Data\\FrameNet\\\\nltk_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for loading wiki articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Key functions\n",
    "\n",
    "# Check to see if given wiki article can be loaded, needed to avoid crashes for loading non-existent articles\n",
    "def check_wiki_article(title, printing=False):\n",
    "    \"\"\" string -> bool\n",
    "    Returns True if the article corresponding to the inputted title can be loaded, False if not.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = wikipedia.page(title) # load the wiki article\n",
    "        if printing==True:\n",
    "            print('Loaded: '+article.title)\n",
    "            print(article.content[0:100]) # show preview of article\n",
    "        loaded = True\n",
    "    except:\n",
    "        loaded = False\n",
    "    \n",
    "    if loaded==False:\n",
    "        try:\n",
    "            alt_title = title+'s' # plural sometimes works\n",
    "            article = wikipedia.page(alt_title)\n",
    "            if printing==True:\n",
    "                print('Loaded: '+article.title)\n",
    "                print('Title used: '+alt_title)\n",
    "                print(article.content[0:100]) # print a preview\n",
    "            loaded = True\n",
    "        except:\n",
    "            loaded = False\n",
    "    \n",
    "    if loaded==False:\n",
    "        try:\n",
    "            alt_title = title+title[-1] # sometimes this works for some reason\n",
    "            article = wikipedia.page(alt_title)\n",
    "            if printing==True:\n",
    "                print('Loaded: '+article.title)\n",
    "                print('Title used: '+alt_title)\n",
    "                print(article.content[0:100]) # print a preview\n",
    "            loaded = True\n",
    "        except:\n",
    "            if printing==True: \n",
    "                print('not found')\n",
    "            loaded = False\n",
    "            \n",
    "    return(loaded)\n",
    "\n",
    "\n",
    "# Load plain text of single wiki article\n",
    "def load_wiki_article(article_title):\n",
    "    \"\"\" string -> list\n",
    "    Loads the wikipedia article corresponding to the given title, returning its content as a list of sentences.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = wikipedia.page(article_title) # load the wiki article\n",
    "        article_sentences = split_to_list(article)\n",
    "    except:\n",
    "        try:\n",
    "            alt_title = article_title+'s' # plural sometimes works\n",
    "            article = wikipedia.page(alt_title)\n",
    "            article_sentences = split_to_list(article)\n",
    "        except:\n",
    "            print(article_title+' not found')\n",
    "            article_sentences = [] # return empty list\n",
    "    finally:\n",
    "        return(article_sentences)\n",
    "\n",
    "\n",
    "# Split article content into list of one sentence per line\n",
    "def split_to_list(article):\n",
    "    \"\"\" article_object -> list\n",
    "    Takes a wikipedia article object and extracts the contents as text, splitting to one sentence per line \n",
    "    and removing some irrelevant punctuation and short sentences. Returns a list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(article.content, language=\"english\") # split article by paragraph\n",
    "    sentences_final = []\n",
    "    skip=False\n",
    "    skip_set = ('i.e.','e.g.')\n",
    "    min_sentence_len = 50 # ignore sentences with fewer characters\n",
    "    i=0\n",
    "\n",
    "    # Process by sentence\n",
    "    for sentence in sentences:\n",
    "        if skip==True: # skip if needed\n",
    "            skip=False\n",
    "            i=i+1\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            l = len(sentence)\n",
    "            if sentence[l-4:l] in skip_set: # if last four chars match anything in the skip set (e.g. or i.e.)\n",
    "                sentence = sentence+' '+sentences[i+1] # combine with next line\n",
    "                skip=True # skip the next line as we just added it on to this line\n",
    "\n",
    "            # Processing of article text to remove metadata and irrelevant lines\n",
    "            sentence = re.sub('\\[.+\\]', '', sentence) # remove anything in square brackets (mostly the pronunciation guide)\n",
    "            sentence = re.sub('(\\W);', '\\\\1', sentence)\n",
    "            sentence = re.sub('([a-z]{2,}\\.)([A-Z][a-z])', '\\\\1\\n\\\\2', sentence) # split the weird sentences with .New format\n",
    "            sentence = re.sub(':\\s(\\d|,|\\s|\\â€“){2,}', '', sentence) # remove lingering page numbers\n",
    "            new_sentences = sentence.split('\\n') # split multi-line paragraphs\n",
    "\n",
    "            for new_sentence in new_sentences:\n",
    "                if new_sentence=='': # remove blank lines\n",
    "                    continue\n",
    "                elif new_sentence[0]=='=': # remove headings\n",
    "                    continue\n",
    "                elif len(new_sentence)<min_sentence_len: # remove very short lines\n",
    "                    continue\n",
    "                elif new_sentence[-1]!='.': # must end with full stop\n",
    "                    continue\n",
    "                elif new_sentence.find('ISBN ')>0: # ignore lines with ISBNs\n",
    "                    continue\n",
    "                else:\n",
    "                    new_sentence = new_sentence.replace('\"','') # remove quotation marks\n",
    "                    sentences_final.append(new_sentence)\n",
    "            i=i+1\n",
    "\n",
    "    print('Loaded: '+article.title+', Sentences: '+str(len(sentences_final))) # number of sentences\n",
    "    return(sentences_final)\n",
    "\n",
    "\n",
    "# Save list of stentences from a given article to a file\n",
    "def save_sentences(sentences_list, filename, path):\n",
    "    \"\"\" list_str, str, str -> None\n",
    "    Saves a list of sentences to a specified filename.\n",
    "    \"\"\"\n",
    "    save_path = path+filename+'.txt'\n",
    "    save_file = open(save_path, \"a\", encoding='utf-8')\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        #print(sentence)\n",
    "        save_file.writelines(sentence)\n",
    "        save_file.write('\\n')            \n",
    "    save_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load list of articles and save the sentences to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 10001 articles loaded\n"
     ]
    }
   ],
   "source": [
    "# Load list of wikipedia articles to use\n",
    "titles_file = path_base+'Corpus Data\\Wikipedia 10k corpus\\\\article_list.txt'\n",
    "article_titles_pd = pd.read_table(titles_file, index_col=0, header=None, quoting=csv.QUOTE_NONE, skip_blank_lines=True)\n",
    "article_titles_list = article_titles_pd.index.values\n",
    "print('List of '+str(len(article_titles_list))+' articles loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Chemistry, Sentences: 260\n"
     ]
    }
   ],
   "source": [
    "# Trial loading and printing article\n",
    "article_title = 'chemistry'\n",
    "if check_wiki_article(article_title):\n",
    "    article_content = load_wiki_article(article_title)\n",
    "else:\n",
    "    print('not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load plain text of all wiki articles from list and save sentences to file\n",
    "save_path = path_base+'/Corpus Data/'\n",
    "for article_title in article_titles_list[0:5]:\n",
    "    sentences_list = load_wiki_article(article_title)\n",
    "    save_sentences(sentences_list, 'full_corpus', save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
