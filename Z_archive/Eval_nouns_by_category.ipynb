{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "np.set_printoptions(precision=4, threshold=1000, linewidth=10000, suppress=True, floatmode='fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load lists of files\n",
    "\n",
    "# Get list of embedding models\n",
    "folder_loc= 'D:/Study and Projects/School Work/Year 25 - PhD 1/Data//'\n",
    "model_loc = 'Word Embeddings//'\n",
    "model_files = {\n",
    "                  'CW_vectors':'Collobert and Weston Vectors/embeddings.txt',\n",
    "                  'word2vec_skip':'Word2vec Skipgram CoNLL17/model_mini.txt',\n",
    "                  'gensim_skip':'Gensim Skipgram wiki+giga/model_mini.txt',\n",
    "                  'gensim_BNC':'Gensim Skipgram BNC/model_mini.txt',\n",
    "                  'gensim_cbow':'Gensim CBoW giga/2010_mini.txt',\n",
    "                  'glove':'Glove Word Embeddings/glove.840B.300d.mini.txt',\n",
    "                  'fasttext':'FastText Skipgram wiki+giga/model_mini.txt',\n",
    "                  'elmo':'Elmo Embeddings/elmo_mini.txt',\n",
    "                  'conceptnet':'ConceptNet Embeddings/numberbatch-en.txt',\n",
    "                  'wordnet':'WordNet Word Embeddings/wn2vec_mini.txt',\n",
    "                  'bert_large':'bert_large_uncased_mini.txt',\n",
    "                  'gpt2_large':'gpt2_large_mini.txt',\n",
    "                  'electra_large':'electra_large_mini.txt',\n",
    "                  'albert_xxlarge':'albert-xxlarge-v2_mini.txt',\n",
    "                  'sembert':'sembert_mini.txt',\n",
    "                  'ernie_base_0':'Ernie Base Embeddings/ernie-2.0-en-layer-0.txt',\n",
    "                  'ernie_context_5':'Ernie Wikipedia Embeddings/Generic Embeddings/contextual_embeddings_layer_normalised_5.txt',\n",
    "                  'ernie_context_5_v':'Ernie Wikipedia Embeddings/Verb Embeddings/contextual_embeddings_layer_normalised_5.txt',\n",
    "                  'ernie_context_5_n':'Ernie Wikipedia Embeddings/Noun Embeddings/contextual_embeddings_layer_normalised_5.txt'\n",
    "               }\n",
    "\n",
    "# Get list of empirical datasets\n",
    "directory = folder_loc+'Word Similarity Data//Leuven Natural Concept Database//pairwise similarities' # LNCD data\n",
    "# directory = folder_loc+'Word Similarity Data//Lee Behavioral Data Repository//pairwise similarities' # Lee data\n",
    "# directory = folder_loc+'Word Similarity Data//Spatial Arrangement Method Data//study1_data//pairwise similarities' # SpAM data\n",
    "dataset_files = {}\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    dataset_name,file_type = filename.split('.')\n",
    "    if os.path.isfile(f) and file_type=='txt': # get only .txt files\n",
    "        dataset_files[dataset_name] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define key functions\n",
    "def import_model(model_name):\n",
    "    \"\"\" string -> None\n",
    "    Imports an embedding model, storing it in the model_embed_storage dictionary.\n",
    "    \"\"\"\n",
    "        \n",
    "    # open relevant file\n",
    "    file_loc = model_files[model_name]\n",
    "    filename = folder_loc+model_loc+file_loc\n",
    "    with open(filename, encoding='utf-8') as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    model_dict = {} # create word dictionary for specific model\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        embedding_list = [float(x) for x in word_list[1:-1]] # store embeddings\n",
    "        embedding_np = np.array(embedding_list)\n",
    "        model_dict[word] = embedding_np\n",
    "\n",
    "    model_embed_storage[model_name] = model_dict # store model dictionary in the models dictionary\n",
    "    print(model_name+' loaded')\n",
    "    \n",
    "\n",
    "# Function to load word similarity data for specified dataset\n",
    "def import_dataset(dataset_name):\n",
    "    \"\"\" string -> None\n",
    "    Imports a dataset, storing a value of the form (list, numpy_array) in the dataset_storage dictionary.\n",
    "    \"\"\"\n",
    "    file_loc = dataset_files[dataset_name]\n",
    "    with open(file_loc, encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    wordpairs = [None]*len(lines) # initialise storage\n",
    "    ratings = [None]*len(lines)\n",
    "    i=0\n",
    "    for line in lines:\n",
    "        line = line.strip() # remove new line chars\n",
    "        wordpairs[i] = line.split() # split at any whitespace chars\n",
    "        ratings[i] = float(wordpairs[i][2])\n",
    "        wordpair_str = wordpairs[i][0]+' '+wordpairs[i][1]\n",
    "        i=i+1\n",
    "    ratings = np.array(ratings)\n",
    "\n",
    "    dataset_storage[dataset_name] = (wordpairs, ratings)\n",
    "    \n",
    "    \n",
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_sim(embed_1, embed_2):\n",
    "    \"\"\" numpy_array, numpy_array -> float\n",
    "    Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "    \"\"\"\n",
    "    if np.dot(embed_1,embed_2) == 0:\n",
    "        similarity = 0 # don't normalise if similarity is zero\n",
    "    else:\n",
    "        similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "        #similarity, _ = spearmanr(embed_1, embed_2)\n",
    "    return(similarity)\n",
    "\n",
    "\n",
    "# Function to compute the correlation between a given set of model and dataset embedding similarities\n",
    "def compute_embed_correls(dataset_similarities, model_similarities,printing=False):\n",
    "    \"\"\" list_flt, list_flt, int, boolean -> (list_flt, list_flt, list_flt)\n",
    "    Computes the pearson_r and spearman_r between word similarities for a dataset and model.\n",
    "    \"\"\"\n",
    "    pearson_r = np.corrcoef(dataset_similarities, model_similarities)[0,1]\n",
    "    spearman_r, p = spearmanr(dataset_similarities, model_similarities)\n",
    "    # differences = np.array(model_similarities)-np.array(dataset_similarities)# model minus dataset\n",
    "            \n",
    "    if printing==True: # printing results\n",
    "        print('pearson: {:.3f}'.format(pearson_r), '\\nspearman: {:.3f}\\n'.format(spearman_r))\n",
    "    return(pearson_r, spearman_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset pairs and sims\n",
    "dataset_storage = {}\n",
    "for dataset in dataset_files:\n",
    "    import_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conceptnet loaded\n"
     ]
    }
   ],
   "source": [
    "## Load word embeddings\n",
    "model_embed_storage = {}\n",
    "# embedding_model = 'ernie_base_0'\n",
    "embedding_model = 'conceptnet'\n",
    "import_model(embedding_model)\n",
    "word_embeds = model_embed_storage[embedding_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LNCD_all: words=380, found=380\n",
      "LNCD_appliances: words=30, found=30\n",
      "LNCD_birds: words=29, found=29\n",
      "LNCD_clothes: words=28, found=28\n",
      "LNCD_fish: words=21, found=21\n",
      "LNCD_fruits: words=29, found=29\n",
      "LNCD_insects: words=23, found=23\n",
      "LNCD_mammals: words=29, found=29\n",
      "LNCD_music: words=22, found=22\n",
      "LNCD_occupations: words=29, found=29\n",
      "LNCD_reptiles: words=18, found=18\n",
      "LNCD_sports: words=27, found=27\n",
      "LNCD_tools: words=25, found=25\n",
      "LNCD_vegetables: words=27, found=27\n",
      "LNCD_vehicles: words=27, found=27\n",
      "LNCD_weapons: words=19, found=19\n"
     ]
    }
   ],
   "source": [
    "## Store word embeddings for all vocab by category\n",
    "\n",
    "# Define dictionary of words needing replacement\n",
    "replace_words = {'trolleycar':'streetcar', 't-shirt':'t_shirt', 'shot-put':'shot_put', 'go-cart':'go_cart'}\n",
    "\n",
    "# Loop over all categories in the dataset\n",
    "model_word_embeds_by_cat = {}\n",
    "vocab_set_by_cat = {}\n",
    "for category in dataset_files:\n",
    "    word_embeds_store = []\n",
    "\n",
    "    # Loop over all vocab in catagory\n",
    "    vocab_set = set(np.array(dataset_storage[category][0])[:,0])\n",
    "    vocab_set_by_cat[category] = vocab_set\n",
    "    for word in vocab_set:\n",
    "        if word in replace_words.keys(): # words absent from conceptnet\n",
    "            word=replace_words[word]\n",
    "\n",
    "        try:\n",
    "            word_embed = word_embeds[word]\n",
    "            word_embeds_store.append(word_embed)\n",
    "        except KeyError: # for missing words\n",
    "            print('missing '+word+' in '+category)\n",
    "            continue\n",
    "    \n",
    "    # Store word embeddings\n",
    "    model_word_embeds_by_cat[category] = np.array(word_embeds_store)\n",
    "    \n",
    "# Check for any missing words\n",
    "for category in model_word_embeds_by_cat.keys():\n",
    "    print(category+': words='+str(len(vocab_set_by_cat[category]))+', found='+str(model_word_embeds_by_cat[category].shape[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute model similarities and correlate with dataset similarities\n",
    "\n",
    "# Loop over all categories in the dataset\n",
    "model_dataset_sims_dict = {}\n",
    "for category in dataset_files:\n",
    "    model_dataset_sims = {}\n",
    "    \n",
    "    # This word will be used to modify the other words in the dataset\n",
    "    category_name = category.split('_')[1]\n",
    "    if category_name=='all': # replace with generic noun\n",
    "        category_name='man'\n",
    "    cat_word_embed = word_embeds[category_name]\n",
    "    mean_cat_embed = model_word_embeds_by_cat[category].mean(axis=0)\n",
    "    var_cat_embed = model_word_embeds_by_cat[category].var(axis=0)\n",
    "    modifications = {\"None\": 0, \"cat_embed\": cat_word_embed, \"mean_cat\": mean_cat_embed, \"var_cat\": var_cat_embed} # addition version\n",
    "    # modifications = {\"None\": 2, \"cat_embed\": cat_word_embed, \"mean_cat\": mean_cat_embed, \"var_cat\": var_cat_embed} # multiplication version\n",
    "    \n",
    "    # Loop over all modifiers for embeddings\n",
    "    for mod in modifications.keys():\n",
    "        model_dataset_sims_single_mod = {}\n",
    "        \n",
    "        # Loop over all wordpairs in catagory\n",
    "        for word_pair in dataset_storage[category][0]:\n",
    "            word_1 = word_pair[0]\n",
    "            word_2 = word_pair[1]\n",
    "            dataset_sim = float(word_pair[2])\n",
    "            \n",
    "            # Calculate cosine similarities\n",
    "            try:\n",
    "                embed_1 = word_embeds[word_1]+0.9*modifications[mod]\n",
    "                embed_2 = word_embeds[word_2]+0.9*modifications[mod]\n",
    "                model_sim = cosine_sim(embed_1,embed_2)\n",
    "                model_dataset_sims_single_mod[word_1+' '+word_2] = [dataset_sim,model_sim]\n",
    "            except KeyError: # for missing words\n",
    "                continue\n",
    "        \n",
    "        # Store results for a single modifier\n",
    "        model_dataset_sims[mod] = model_dataset_sims_single_mod\n",
    "    \n",
    "    # Store category sim data in dictionary\n",
    "    model_dataset_sims_dict[category] = model_dataset_sims\n",
    "    \n",
    "# Compute model-dataset correlations\n",
    "correl_storage = {}\n",
    "for dataset_cat in model_dataset_sims_dict.keys():\n",
    "    results_single_cat = []\n",
    "    for mod in modifications.keys():\n",
    "        model_dataset_sims_np = np.array(list(model_dataset_sims_dict[dataset_cat][mod].values())).transpose()\n",
    "        dataset_sims = model_dataset_sims_np[0]\n",
    "        model_sims = model_dataset_sims_np[1]\n",
    "        results_single_cat.append(compute_embed_correls(dataset_sims,model_sims)[1]) # get spearman correl\n",
    "    correl_storage[dataset_cat] = np.array(results_single_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SpAM_birds': array([0.4869, 0.4583, 0.4680, 0.4865]),\n",
       " 'SpAM_clothing': array([0.6521, 0.6128, 0.6163, 0.6509]),\n",
       " 'SpAM_fruit': array([0.5250, 0.5244, 0.5206, 0.5257]),\n",
       " 'SpAM_furniture': array([0.7038, 0.6039, 0.6467, 0.7061]),\n",
       " 'SpAM_professions': array([0.6211, 0.5707, 0.6021, 0.6217]),\n",
       " 'SpAM_sports': array([0.5065, 0.4480, 0.4494, 0.5067]),\n",
       " 'SpAM_vegetables': array([0.4841, 0.4321, 0.4687, 0.4843]),\n",
       " 'SpAM_vehicles': array([0.7994, 0.7735, 0.7812, 0.8050])}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correl_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LBDR_all': array([0.2967, 0.3023, 0.2874, 0.2963]),\n",
       " 'lee_animals': array([0.5259, 0.4993, 0.4813, 0.5291]),\n",
       " 'lee_clothing': array([-0.0625, -0.0423, -0.0503, -0.0650]),\n",
       " 'lee_fish': array([0.3502, 0.3341, 0.3368, 0.3497]),\n",
       " 'lee_fruit': array([0.3487, 0.3470, 0.3490, 0.3487]),\n",
       " 'lee_furniture': array([0.4379, 0.3245, 0.3611, 0.4451]),\n",
       " 'lee_kinship': array([0.6738, 0.6729, 0.6662, 0.6726]),\n",
       " 'lee_mixture': array([0.1669, 0.0694, 0.1718, 0.1708]),\n",
       " 'lee_sport': array([0.6104, 0.5844, 0.5744, 0.6116]),\n",
       " 'lee_tools': array([0.3379, 0.2891, 0.3017, 0.3373]),\n",
       " 'lee_vegetables': array([0.3371, 0.3072, 0.3180, 0.3353]),\n",
       " 'lee_vehicles': array([0.4636, 0.4608, 0.4405, 0.4641]),\n",
       " 'lee_weapons': array([0.5561, 0.4437, 0.5014, 0.5502])}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correl_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LNCD_all': array([0.3797, 0.3850, 0.3696, 0.3854]),\n",
       " 'LNCD_appliances': array([0.5871, 0.4914, 0.5668, 0.5878]),\n",
       " 'LNCD_birds': array([0.4762, 0.4552, 0.4522, 0.4775]),\n",
       " 'LNCD_clothes': array([0.5231, 0.4987, 0.5093, 0.5235]),\n",
       " 'LNCD_fish': array([0.3332, 0.3426, 0.3168, 0.3344]),\n",
       " 'LNCD_fruits': array([0.2944, 0.2668, 0.2741, 0.2947]),\n",
       " 'LNCD_insects': array([0.3135, 0.2763, 0.2766, 0.3110]),\n",
       " 'LNCD_mammals': array([0.5814, 0.5318, 0.5479, 0.5781]),\n",
       " 'LNCD_music': array([0.4405, 0.4320, 0.4249, 0.4413]),\n",
       " 'LNCD_occupations': array([0.5367, 0.4087, 0.5430, 0.5315]),\n",
       " 'LNCD_reptiles': array([0.3565, 0.3341, 0.3412, 0.3529]),\n",
       " 'LNCD_sports': array([0.4420, 0.4076, 0.4081, 0.4428]),\n",
       " 'LNCD_tools': array([0.5139, 0.4969, 0.5139, 0.5150]),\n",
       " 'LNCD_vegetables': array([0.2374, 0.2110, 0.2185, 0.2362]),\n",
       " 'LNCD_vehicles': array([0.7026, 0.6881, 0.6822, 0.7119]),\n",
       " 'LNCD_weapons': array([0.5932, 0.4591, 0.5101, 0.5928])}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correl_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e86f3c5951539f2722badccf95389aafd2846e6c6d91d84b623110a1b2c6697"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
