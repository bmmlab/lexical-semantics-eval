{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "file_path = 'D:/Study and Projects/School Work/Year 25 - PhD 1/Data/Word Embeddings//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to import a word embedding model from a file\n",
    "def import_model(model_name, full_import=False, vocab_set=[]):\n",
    "    \"\"\" string -> None\n",
    "    Imports an embedding model, storing it in the model_embed_storage dictionary.\n",
    "    \"\"\"\n",
    "        \n",
    "    # open relevant file\n",
    "    filename = file_path+model_name\n",
    "    with open(filename) as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    model_dict = {} # create word dictionary for specific model\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        if full_import==False and word in vocab_set: # only  words for testing if full_import==False\n",
    "            embedding_list = [float(x) for x in word_list[1:-1]] # store embeddings\n",
    "            embedding_np = np.array(embedding_list)\n",
    "            model_dict[word] = embedding_np\n",
    "        elif full_import==True: # this will import all words in the vocab set, not just those for testing\n",
    "            embedding_list = [float(x) for x in word_list[1:-1]] # store embeddings\n",
    "            embedding_np = np.array(embedding_list)\n",
    "            model_dict[word] = embedding_np\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return(model_dict)\n",
    "\n",
    "\n",
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_sim(embed_1, embed_2):\n",
    "    \"\"\" numpy_array, numpy_array -> float\n",
    "    Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "    \"\"\"\n",
    "    if np.dot(embed_1,embed_2) == 0:\n",
    "        similarity = 0 # don't normalise if similarity is zero\n",
    "    else:\n",
    "        similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "        #similarity, _ = spearmanr(embed_1, embed_2)\n",
    "    return(similarity)\n",
    "\n",
    "\n",
    "# Function to load word similarity data for specified dataset\n",
    "def import_dataset(dataset_name):\n",
    "    \"\"\" string -> None\n",
    "    Imports a dataset, storing a value of the form (list, numpy_array) in the dataset_storage dictionary.\n",
    "    \"\"\"\n",
    "    file_loc = 'D:/Study and Projects/School Work/Year 25 - PhD 1/Data/Word Similarity Data/Word Similarities Final//'\n",
    "    filename = file_loc+dataset_name\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    wordpairs = [None]*len(lines) # initialise storage\n",
    "    ratings = [None]*len(lines)\n",
    "    i=0\n",
    "    for line in lines:\n",
    "        line = line.strip() # remove new line chars\n",
    "        wordpairs[i] = line.split() # split at any whitespace chars\n",
    "        ratings[i] = float(wordpairs[i][2])\n",
    "        wordpair_str = wordpairs[i][0]+' '+wordpairs[i][1]\n",
    "        i=i+1\n",
    "    ratings = np.array(ratings)\n",
    "\n",
    "    return(wordpairs,ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147307\n"
     ]
    }
   ],
   "source": [
    "# Load sense embeddings\n",
    "model_name = 'ARES Embeddings/ares_bert_large_english.txt'\n",
    "embeds = import_model(model_name, full_import=True)\n",
    "word_sense_list = list(embeds.keys())\n",
    "\n",
    "# Load wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.data import path # need to specify the location of the nltk data\n",
    "path.append(\"D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\Frames and Structured Data\\\\nltk_data\")\n",
    "\n",
    "# Construct a dictionary of words from embeddings file with all their wordnet senses\n",
    "word_sense_dict = {}\n",
    "for word_sense in word_sense_list:\n",
    "    word = word_sense.split('%')[0] # get base word\n",
    "    try: # add subsequent senses to dictionary\n",
    "        word_sense_dict[word].append(word_sense)\n",
    "    except KeyError: # add first element\n",
    "        word_sense_dict[word] = [word_sense]\n",
    "\n",
    "print(len(word_sense_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45938633790649047\n"
     ]
    }
   ],
   "source": [
    "# Import word similarity dataset and calculate correlation between dataset and sense embeddings\n",
    "simverb = import_dataset('EN-SimVerb-3200-mod.txt')\n",
    "calc_sims = []\n",
    "expr_sims = []\n",
    "\n",
    "for word_pair in simverb[0]:\n",
    "    if word_pair[0] in word_sense_dict.keys() and word_pair[1] in word_sense_dict.keys():\n",
    "        word_1 = word_pair[0]\n",
    "        word_2 = word_pair[1]\n",
    "\n",
    "        calc_sims_temp = [] # temporary storage\n",
    "        for word_1_sense in word_sense_dict[word_1]:\n",
    "            for word_2_sense in word_sense_dict[word_2]:\n",
    "                sense_sim = cosine_sim(embeds[word_1_sense],embeds[word_2_sense])\n",
    "                calc_sims_temp.append(sense_sim)\n",
    "        \n",
    "        # print(word_1_sense,word_2_sense,sense_sim)\n",
    "        calc_sims.append(np.mean(calc_sims_temp))\n",
    "        expr_sims.append(float(word_pair[2]))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "spearman_r, p = spearmanr(calc_sims, expr_sims)\n",
    "print(spearman_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admire%2:39:00:: like%2:37:01:: 0.696030222379393\n",
      "admire%2:39:00:: like%2:37:04:: 0.790853941551968\n",
      "admire%2:39:00:: like%2:37:05:: 0.7961704219091083\n",
      "admire%2:39:00:: like%2:37:06:: 0.6848265046387817\n",
      "admire%2:39:00:: like%1:09:01:: 0.7021524368519955\n",
      "admire%2:39:00:: like%2:31:00:: 0.6488576485727974\n",
      "admire%2:39:00:: like%1:09:00:: 0.6962509991598497\n",
      "admire%2:39:00:: like%3:00:04:: 0.5908880032517394\n",
      "admire%2:39:00:: like%3:00:00:: 0.6379227243642132\n",
      "admire%2:39:00:: like%3:00:02:: 0.5443994644041646\n",
      "admire%2:39:00:: like%5:00:00:same:00 0.6742458614572683\n",
      "admire%2:37:00:: like%2:37:01:: 0.7154149842796691\n",
      "admire%2:37:00:: like%2:37:04:: 0.8526063352951069\n",
      "admire%2:37:00:: like%2:37:05:: 0.8343727944158179\n",
      "admire%2:37:00:: like%2:37:06:: 0.7002974664207727\n",
      "admire%2:37:00:: like%1:09:01:: 0.7263593137316249\n",
      "admire%2:37:00:: like%2:31:00:: 0.6884834533866895\n",
      "admire%2:37:00:: like%1:09:00:: 0.7129674077552316\n",
      "admire%2:37:00:: like%3:00:04:: 0.6240282241184786\n",
      "admire%2:37:00:: like%3:00:00:: 0.6425129226087652\n",
      "admire%2:37:00:: like%3:00:02:: 0.5610825825174284\n",
      "admire%2:37:00:: like%5:00:00:same:00 0.6843289734817577\n"
     ]
    }
   ],
   "source": [
    "# Compare all senses of two words\n",
    "for word_1_sense in word_sense_dict['admire']:\n",
    "    for word_2_sense in word_sense_dict['like']:\n",
    "        sense_sim = cosine_sim(embeds[word_1_sense],embeds[word_2_sense])\n",
    "        print(word_1_sense, word_2_sense, sense_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feel admiration for'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma_from_key(\"admire%2:37:00::\").synset().definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wish harm upon; invoke evil upon'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('curse.v.03').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results to a file\n",
    "save_file = open('D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\Analysis Results\\SimVerb_mod_ARES_sense_embed.txt', \"a\", encoding='utf-8')\n",
    "all_data = np.column_stack([calc_sims, expr_sims])\n",
    "i=0\n",
    "for line in all_data:\n",
    "    save_file.writelines(simverb[0][i][0]+' '+simverb[0][i][1]+','+str(line[0])+','+str(line[1]))\n",
    "    save_file.write('\\n')\n",
    "    i=i+1\n",
    "save_file.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
