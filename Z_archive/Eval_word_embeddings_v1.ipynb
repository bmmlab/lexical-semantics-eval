{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b06582-ba94-48c2-a4b9-061db7f14af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import simple_elmo\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from scipy.stats import spearmanr\n",
    "from simple_elmo import ElmoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e75d6eb8-f1fd-43c8-af42-9383e4a02eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_analysis(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = list(self.model_storage.keys())\n",
    "        self.datasets = list(self.dataset_storage.keys())\n",
    "        return(None)\n",
    "    \n",
    "    # Define file location variables\n",
    "    folder_loc= 'D:/Study and Projects/School Work/Year 25 - PhD 1/PhD Work/Data//'\n",
    "    model_loc = 'Word Embeddings//'\n",
    "    dataset_loc = 'Word Similarity Data/Collection of Word Similarity Benchmarks//'\n",
    "    path_base = 'D:/Study and Projects/School Work/Year 25 - PhD 1/PhD Work/Data/Sentence Embeddings' #must use '/'\n",
    "\n",
    "    model_files = {'glove':'Glove Word Embeddings\\glove.6B.300d.txt',\n",
    "                   'fasttext':'fastText_Skipgram_true_wiki.txt',\n",
    "                   'conceptnet':'conceptnet-numberbatch-300-en.txt',\n",
    "                   'word2vec_skip':'Word2vec Skipgram CoNLL17\\model.txt',\n",
    "                   'lexvec':'lexvec_embeddings_wiki+newscrawl_300d.txt',\n",
    "                   'bert':'bert-base-uncased-vocab.txt',\n",
    "                   'elmo':'Elmo Embeddings\\elmo_false_wiki2019',\n",
    "                  }\n",
    "        \n",
    "    dataset_files = {'RG65':'EN-RG-65.txt',\n",
    "                     'YP130':'EN-YP-130.txt',\n",
    "                     'MTurk287':'EN-MTurk-287.txt',\n",
    "                     'MTurk771':'EN-MTurk-771.txt',\n",
    "                     'WS353':'EN-WS-353-ALL.txt',\n",
    "                     'RW':'EN-RW-STANFORD.txt',\n",
    "                     'MEN':'EN-MEN-TR-3k.txt',\n",
    "                     'SimVerb':'EN-SimVerb-3500.txt',\n",
    "                     'Simlex':'EN-SIMLEX-999.txt'\n",
    "                    }\n",
    "    \n",
    "    model_storage = {'glove':0, 'fasttext':0, 'conceptnet':0, \n",
    "                     'word2vec_skip':0, 'lexvec':0, 'bert':0, 'elmo':0, 'gensim_skip':0}\n",
    "     \n",
    "    dataset_storage = {'RG65':0, 'YP130':0, 'MTurk287':0, 'MTurk771':0, \n",
    "                       'WS353':0, 'RW':0, 'MEN':0, 'SimVerb':0, 'Simlex':0}\n",
    "    \n",
    "        \n",
    "    # Function to load a specific word embedding model\n",
    "    def import_model(self, model_name):\n",
    "        \"\"\" string -> None\n",
    "        Imports an embedding model, storing it in the model_storage dictionary.\n",
    "        \"\"\"\n",
    "        if self.model_storage[model_name]==0:\n",
    "            if model_name=='bert':\n",
    "                model_path = self.folder_loc + 'Sentence Embeddings/bert-base-uncased'\n",
    "                config_state = AutoConfig.from_pretrained(model_path, output_hidden_states=True) # get hidden states\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "                model = AutoModel.from_pretrained(model_path, config=config_state)\n",
    "                self.model_storage[model_name] = [(model,tokenizer)] # need the model and tokeniser\n",
    "                           \n",
    "            elif model_name=='elmo':\n",
    "                options_path = folder_loc + model_loc + 'Elmo Embeddings/elmo_false_wiki2019/options.json'\n",
    "                weights_path = folder_loc + model_loc + 'Elmo Embeddings/elmo_false_wiki2019/model.hdf5'\n",
    "                elmo = Elmo(options_path, weights_path, 3, dropout=0)\n",
    "                self.model_storage[model_name] = [elmo]\n",
    "      #          model_path = self.folder_loc + self.model_loc + 'Elmo Embeddings/elmo_false_wiki2019'\n",
    "      #          graph = tf.Graph()\n",
    "      #          with graph.as_default() as elmo_graph:\n",
    "      #              elmo_model = ElmoModel()\n",
    "      #              elmo_model.load(model_path)\n",
    "      #          with elmo_graph.as_default() as current_graph: # need this part so we can load multiple times\n",
    "      #              tf_session = tf.compat.v1.Session(graph=elmo_graph) # TF_session must be passed with the model\n",
    "      #              with tf_session.as_default() as sess:\n",
    "      #                  elmo_model.elmo_sentence_input = simple_elmo.elmo.weight_layers(\"input\", elmo_model.sentence_embeddings_op)\n",
    "      #                  sess.run(tf.compat.v1.global_variables_initializer())\n",
    "      #          self.model_storage[model_name] = (elmo_model, tf_session)\n",
    "                        \n",
    "            else: # for static word embedding models\n",
    "                file_loc = self.model_files[model_name]\n",
    "                filename = self.folder_loc+self.model_loc+file_loc\n",
    "                self.model_storage[model_name] = [pd.read_table(filename, sep=' ', index_col=0, header=None, quoting=csv.QUOTE_NONE)]\n",
    "        \n",
    "            print(model_name+' loaded')\n",
    "            \n",
    "    \n",
    "    # Function to load word similarity data for specified dataset\n",
    "    def import_dataset(self, dataset_name):\n",
    "        \"\"\" string -> None\n",
    "        Imports a dataset, storing a value of the form (list, numpy_array) in the dataset_storage dictionary.\n",
    "        \"\"\"\n",
    "        if self.dataset_storage[dataset_name]==0: # if dataset not yet loaded\n",
    "            file_loc = self.dataset_files[dataset_name]\n",
    "            filename = self.folder_loc+self.dataset_loc+file_loc\n",
    "            with open(filename) as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            wordpairs = [None]*len(lines) # initialise storage\n",
    "            ratings = [None]*len(lines)\n",
    "            i=0\n",
    "            for line in lines:\n",
    "                line = line.strip() # remove new line chars\n",
    "                wordpairs[i] = line.split() # split at any whitespace chars\n",
    "                ratings[i] = float(wordpairs[i][2])\n",
    "                i=i+1\n",
    "            ratings = np.array(ratings)\n",
    "\n",
    "            self.dataset_storage[dataset_name] = (wordpairs, ratings)\n",
    "            print(dataset_name+' loaded')   \n",
    "        \n",
    "    \n",
    "    # Function to get the embedding for a specific word, given a model\n",
    "    def get_word_embed(self, model_name, word, layer, comp_method):\n",
    "        self.import_model(model_name)\n",
    "        model = self.model_storage[model_name][0] # [0] to reach inside the array its in\n",
    "        \n",
    "        if model_name=='bert':\n",
    "            model_main = model[0]\n",
    "            tokenizer = model[1] # transformers need a tokenizer as well\n",
    "            encoded_input = tokenizer(word, return_tensors='pt') #pt = pytorch\n",
    "            model_output = model_main(**encoded_input)\n",
    "            word_embedding_raw = np.array(model_output[2][layer].detach()[0])\n",
    "            # embeddings depend on the comp_method chosen\n",
    "            if comp_method=='mean': # take the mean of all tokens\n",
    "                word_embedding = word_embedding_raw.mean(axis=0)\n",
    "            elif comp_method=='cls': # use the 'CLS' token\n",
    "                word_embedding = word_embedding_raw[0]\n",
    "            elif comp_method=='decontext': # take the mean of word tokens only\n",
    "                word_embedding = word_embedding_raw[1:-1].mean(axis=0)\n",
    "\n",
    "        elif model_name=='elmo':\n",
    "            character_ids = batch_to_ids(list([word]))\n",
    "            embeddings_allennlp = model(character_ids)['elmo_representations'][0]\n",
    "            word_embedding = embeddings_allennlp[0].mean(axis=0).detach().numpy()\n",
    "            #tf_session = self.model_storage[model_name][1] # need TF session information for elmo\n",
    "            #char_embeddings = model.get_elmo_vector_average(word, layers='average', warmup=False, session=tf_session)\n",
    "            #word_embedding = char_embeddings.mean(axis=0)\n",
    "            \n",
    "        else: # for static word embedding models\n",
    "            embed_dim = round(model.loc[['man']].shape[1],-2) # get embedding length\n",
    "            if word in list(model.index.values):\n",
    "                word_embed = model.loc[[word]] # get embedding from pandas array\n",
    "                word_embedding = np.array(word_embed)[0][0:embed_dim]\n",
    "            else: # if the word can't be found in the model\n",
    "                print('missing '+word)\n",
    "                word_embedding = 4*(np.random.rand(1,embed_dim))[0] # random embedding\n",
    "                \n",
    "        return(word_embedding)\n",
    "    \n",
    "    \n",
    "    # Function to compute the similarities of all word pairs from a given database, for a given model\n",
    "    def compute_model_sims(self, model_name, dataset_name, layer, comp_method):\n",
    "        self.import_dataset(dataset_name) # load dataset if needed\n",
    "        dataset_words = self.dataset_storage[dataset_name][0] # word pairs in [0]\n",
    "        \n",
    "        embed_sims = [None]*len(dataset_words)\n",
    "        i=0\n",
    "        for word_pair in dataset_words:\n",
    "            word_embed_1 = self.get_word_embed(model_name, word_pair[0], layer, comp_method)\n",
    "            word_embed_2 = self.get_word_embed(model_name, word_pair[1], layer, comp_method)\n",
    "            embed_sims[i] = self.cosine_sim(word_embed_1, word_embed_2)\n",
    "            i=i+1\n",
    "        return(embed_sims)\n",
    "    \n",
    "    \n",
    "    # Function to calculate cosine similarity between two embeddings\n",
    "    def cosine_sim(self, embed_1, embed_2):\n",
    "        \"\"\" numpy_array, numpy_array -> float\n",
    "        Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "        \"\"\"\n",
    "        if np.dot(embed_1,embed_2) == 0:\n",
    "            similarity = 0 # don't normalise if similarity is zero\n",
    "        else:\n",
    "            similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "        return(similarity)\n",
    "       \n",
    "    \n",
    "    # Function to compute the correlation between model and dataset embedding similarities\n",
    "    def compute_embed_correls(self, model_name, dataset_name, layer, comp_method):\n",
    "        \"\"\" string, string, int, string -> (float, float)\n",
    "        Computes the pearson_r and spearman_r between word similarities for a dataset and model.\n",
    "        \"\"\"\n",
    "        self.import_dataset(dataset_name) # load model and dataset if needed\n",
    "        self.import_model(model_name)\n",
    "        model_sims = self.compute_model_sims(model_name, dataset_name, layer, comp_method)\n",
    "        dataset_sims = self.dataset_storage[dataset_name][1] # similarities stored in [1]\n",
    "        pearson_r = np.corrcoef(model_sims, dataset_sims)[0,1]\n",
    "        spearman_r, p = spearmanr(model_sims, dataset_sims)\n",
    "        return(pearson_r, spearman_r)\n",
    "    \n",
    "    def model_vs_data(self, model_name, dataset_name, layer=0, comp_method='mean'):\n",
    "        correlations = self.compute_embed_correls(model_name, dataset_name, layer, comp_method)\n",
    "        print(\"Evaluating \"+model_name+\" against \"+dataset_name)\n",
    "        print(\"pearson: {:.3f}\".format(correlations[0]), \"\\nspearman: {:.3f}\\n\".format(correlations[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "842428e7-2043-43b0-8e91-2ec454a3eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_analysis = word_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c517a9f3-d80e-4aae-8ecc-eee64efb8592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RG65 loaded\n"
     ]
    }
   ],
   "source": [
    "embedding_analysis.import_dataset('RG65')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b62830a8-4099-474a-9334-a0bc660ce18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 16:21:46,874 : INFO : Initializing ELMo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo loaded\n",
      "Evaluating elmo against RG65\n",
      "pearson: -0.005 \n",
      "spearman: -0.035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_analysis.model_vs_data('elmo','RG65')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23283624-05e0-4cca-85e5-eea00993d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(embed_1, embed_2):\n",
    "    \"\"\" numpy_array, numpy_array -> float\n",
    "    Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "    \"\"\"\n",
    "    if np.dot(embed_1,embed_2) == 0:\n",
    "        similarity = 0 # don't normalise if similarity is zero\n",
    "    else:\n",
    "        similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "    return(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91e3b126-e200-4c07-a645-c25e368b6802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 16:31:02,763 : INFO : Loading model from D:/Study and Projects/School Work/Year 25 - PhD 1/PhD Work/Data//Word Embeddings//Elmo Embeddings/elmo_false_wiki2019...\n",
      "2022-02-18 16:31:02,764 : INFO : We will cache the vocabulary of 100 tokens.\n",
      "D:\\Anaconda\\lib\\site-packages\\simple_elmo\\model.py:529: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  lstm_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:984: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:993: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._bias = self.add_variable(\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:1009: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._proj_kernel = self.add_variable(\n"
     ]
    }
   ],
   "source": [
    "folder_loc= 'D:/Study and Projects/School Work/Year 25 - PhD 1/PhD Work/Data//'\n",
    "model_loc = 'Word Embeddings//'\n",
    "model_path = folder_loc + model_loc + 'Elmo Embeddings/elmo_false_wiki2019'\n",
    "graph = tf.Graph()\n",
    "with graph.as_default() as elmo_graph:\n",
    "    elmo_model = ElmoModel()\n",
    "    elmo_model.load(model_path)\n",
    "with elmo_graph.as_default() as current_graph: # need this part so we can load multiple times\n",
    "    tf_session = tf.compat.v1.Session(graph=elmo_graph) # TF_session must be passed with the model\n",
    "    with tf_session.as_default() as sess:\n",
    "        elmo_model.elmo_sentence_input = simple_elmo.elmo.weight_layers(\"input\", elmo_model.sentence_embeddings_op)\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b52371e-9c53-452d-a20f-eda3c9def8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7654833868676729"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embeddings1 = elmo_model.get_elmo_vectors('cord', layers='top', warmup=False, session=tf_session)\n",
    "word_1 = char_embeddings1.mean(axis=0)[0]\n",
    "char_embeddings2 = elmo_model.get_elmo_vectors('smile', layers='top', warmup=False, session=tf_session)\n",
    "word_2 = char_embeddings2.mean(axis=0)[0]\n",
    "\n",
    "cosine_sim(word_1, word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "358d2fa9-9b78-4418-82bf-4c7be5a22a27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 16:45:56,886 : INFO : Initializing ELMo\n"
     ]
    }
   ],
   "source": [
    "folder_loc= 'D:/Study and Projects/School Work/Year 25 - PhD 1/PhD Work/Data//'\n",
    "model_loc = 'Word Embeddings//'\n",
    "options_path = folder_loc + model_loc + 'Elmo Embeddings/elmo_false_wiki2019/options.json'\n",
    "weights_path = folder_loc + model_loc + 'Elmo Embeddings/elmo_false_wiki2019/model.hdf5'\n",
    "elmo = Elmo(options_path, weights_path, num_output_representations=1, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a91a946-d759-4e45-afa0-9cc2910dfb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7927116"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_ids_1 = batch_to_ids(list(['gem']))\n",
    "embeddings_allennlp_1 = elmo(character_ids_1)['elmo_representations'][0]\n",
    "embed_1 = embeddings_allennlp_1[0].mean(axis=0).detach().numpy()\n",
    "\n",
    "character_ids_2 = batch_to_ids(list(['jewel']))\n",
    "embeddings_allennlp_2 = elmo(character_ids_2)['elmo_representations'][0]\n",
    "embed_2 = embeddings_allennlp_2[0].mean(axis=0).detach().numpy()\n",
    "\n",
    "cosine_sim(embed_1, embed_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08f896f0-0291-43b7-8101-9bb34c46e6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310033"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_ids_1 = batch_to_ids(list(['cord']))\n",
    "embeddings_allennlp_1 = elmo(character_ids_1)['elmo_representations'][0]\n",
    "embed_1 = embeddings_allennlp_1[0].mean(axis=0).detach().numpy()\n",
    "\n",
    "character_ids_2 = batch_to_ids(list(['smile']))\n",
    "embeddings_allennlp_2 = elmo(character_ids_2)['elmo_representations'][0]\n",
    "embed_2 = embeddings_allennlp_2[0].mean(axis=0).detach().numpy()\n",
    "\n",
    "cosine_sim(embed_1, embed_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
