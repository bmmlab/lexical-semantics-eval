{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "path_base = 'D:/Study and Projects/School Work/Year 25 - PhD 1/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for sense embedding analysis\n",
    "\n",
    "# Function to import a word embedding model from a file\n",
    "def import_model(model_name, full_import=False, vocab_set=[]):\n",
    "    \"\"\" string -> None\n",
    "    Imports an embedding model, storing it in the model_embed_storage dictionary.\n",
    "    \"\"\"\n",
    "        \n",
    "    # open relevant file\n",
    "    file_loc = 'Sense Embeddings/Dictionary Sense Embeddings/Combined Embeddings/'\n",
    "    filename = path_base+file_loc+model_name\n",
    "    with open(filename) as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    model_dict = {} # create word dictionary for specific model\n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        if full_import==False and word in vocab_set: # only  words for testing if full_import==False\n",
    "            embedding_list = [float(x) for x in word_list[1:-1]] # store embeddings\n",
    "            embedding_np = np.array(embedding_list)\n",
    "            model_dict[word] = embedding_np\n",
    "        elif full_import==True: # this will import all words in the vocab set, not just those for testing\n",
    "            embedding_list = [float(x) for x in word_list[1:-1]] # store embeddings\n",
    "            embedding_np = np.array(embedding_list)\n",
    "            model_dict[word] = embedding_np\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return(model_dict)\n",
    "\n",
    "\n",
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_sim(embed_1, embed_2):\n",
    "    \"\"\" numpy_array, numpy_array -> float\n",
    "    Returns the cosine similarity (-1 to 1) between two embeddings, inputted as vectors.\n",
    "    \"\"\"\n",
    "    if np.dot(embed_1,embed_2) == 0:\n",
    "        similarity = 0 # don't normalise if similarity is zero\n",
    "    else:\n",
    "        similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n",
    "        #similarity, _ = spearmanr(embed_1, embed_2)\n",
    "    return(similarity)\n",
    "\n",
    "\n",
    "# Function to load word similarity data for specified dataset\n",
    "def import_dataset(dataset_name):\n",
    "    \"\"\" string -> None\n",
    "    Imports a dataset, storing a value of the form (list, numpy_array) in the dataset_storage dictionary.\n",
    "    \"\"\"\n",
    "    file_loc = 'Word Similarity Data/Word Similarities Final/'\n",
    "    filename = path_base+file_loc+dataset_name\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    wordpairs = [None]*len(lines) # initialise storage\n",
    "    ratings = [None]*len(lines)\n",
    "    i=0\n",
    "    for line in lines:\n",
    "        line = line.strip() # remove new line chars\n",
    "        wordpairs[i] = line.split() # split at any whitespace chars\n",
    "        ratings[i] = float(wordpairs[i][2])\n",
    "        wordpair_str = wordpairs[i][0]+' '+wordpairs[i][1]\n",
    "        i=i+1\n",
    "    ratings = np.array(ratings)\n",
    "\n",
    "    return(wordpairs,ratings)\n",
    "\n",
    "# Function to load a specific word embedding model\n",
    "def import_word_model(model_path):\n",
    "    \"\"\" string -> None\n",
    "    Imports an embedding model, storing it in the model_embed_storage dictionary.\n",
    "    \"\"\"\n",
    "    # open relevant file\n",
    "    with open(model_path, encoding='utf-8') as file:\n",
    "        lines = [line.rstrip('\\n') for line in file]\n",
    "    \n",
    "    # create word dictionary for specific model\n",
    "    model_dict = {}  \n",
    "    for line in lines:\n",
    "        word_list = line.split()\n",
    "        word = word_list[0]\n",
    "        embedding_list = [float(x) for x in word_list[1:-1]] # store embeddings\n",
    "        embedding_np = np.array(embedding_list)\n",
    "        model_dict[word] = embedding_np\n",
    "        \n",
    "    return(model_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and calculate correlations of Dictionary SimVerb embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load transformer embeddings preparatory to computing correlation\n",
    "full_word_sense_dict = {}\n",
    "full_word_embeds_dict = {}\n",
    "\n",
    "for transformer_layer in np.arange(1,13):\n",
    "    # Load sense embeddings\n",
    "    model_name = 'normalised_'+str(transformer_layer)+'.txt'\n",
    "    embeds = import_model(model_name, full_import=True)\n",
    "    word_sense_list = list(embeds.keys())\n",
    "\n",
    "    # Construct a dictionary of words from embeddings file with all their dictionary senses\n",
    "    word_sense_dict = {}\n",
    "    for word_sense in word_sense_list:\n",
    "        word = word_sense.split('_')[0] # get base word\n",
    "        try: # add subsequent senses to dictionary\n",
    "            word_sense_dict[word].append(word_sense)\n",
    "        except KeyError: # add first element\n",
    "            word_sense_dict[word] = [word_sense]\n",
    "\n",
    "    # Store in dict\n",
    "    full_word_sense_dict[transformer_layer] = word_sense_dict\n",
    "    full_word_embeds_dict[transformer_layer] = embeds\n",
    "    # print(transformer_layer,len(word_sense_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fods1\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:110: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  warnings.warn(\"The input array could not be properly \"\n"
     ]
    }
   ],
   "source": [
    "## Compute correlation between experimental and sense embeddings\n",
    "\n",
    "# Import word similarity dataset \n",
    "dataset = import_dataset('EN-SimVerb-3200-mod.txt')\n",
    "# dataset = import_dataset('combined_dataset_verbs.txt')\n",
    "all_data_dict = {}\n",
    "correls_dict = {}\n",
    "expr_sims = np.array(np.array(dataset[0])[:,2],dtype=float) # experimental similarities\n",
    "\n",
    "# loop over all layers in transformer\n",
    "for transformer_layer in np.arange(1,13):\n",
    "    all_data = []\n",
    "    expr_sims_included = []\n",
    "    embeds = full_word_embeds_dict[transformer_layer] # get relevant embeddings\n",
    "    word_sense_dict = full_word_sense_dict[transformer_layer] # get word senses\n",
    "\n",
    "    # loop over word pairs in dataset\n",
    "    for word_pair in dataset[0]:\n",
    "        if word_pair[0] in word_sense_dict.keys() and word_pair[1] in word_sense_dict.keys():\n",
    "            word_1 = word_pair[0]\n",
    "            word_2 = word_pair[1]\n",
    "            calc_sims_temp = [] # temporary storage\n",
    "            for word_1_sense in word_sense_dict[word_1]:\n",
    "                for word_2_sense in word_sense_dict[word_2]:\n",
    "                    sense_sim = cosine_sim(embeds[word_1_sense],embeds[word_2_sense])\n",
    "                    calc_sims_temp.append(sense_sim)\n",
    "            \n",
    "            # print(word_1_sense,word_2_sense,sense_sim)\n",
    "            all_data.append((word_pair[0],word_pair[1],np.max(calc_sims_temp),float(word_pair[2]))) # define method to select similarity across senses\n",
    "            expr_sims_included.append(word_pair[2])\n",
    "        else:\n",
    "            if transformer_layer==1:\n",
    "                print(word_pair) # print missing word pairs\n",
    "            continue # skip word pairs without sense embeddings available\n",
    "\n",
    "    # store results\n",
    "    spearman_r, p = spearmanr(np.array(all_data)[:,2], expr_sims_included)\n",
    "    correls_dict[transformer_layer] = spearman_r\n",
    "    all_data_dict[int(transformer_layer)] = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for saving results\n",
    "\n",
    "# Save Dictionary similarities and word pairs to file\n",
    "for transformer_layer in all_data_dict.keys():\n",
    "    data_single_layer = all_data_dict[transformer_layer]\n",
    "    save_file = open(path_base+'\\Analysis Results\\ernie_dictionary_max_'+str(transformer_layer)+'_SimVerb_mod_results.txt', \"w\", encoding='utf-8')\n",
    "    # np.savetxt(save_file, data_single_layer, fmt='%s')\n",
    "    \n",
    "    for line in data_single_layer:\n",
    "        diff = line[3]-line[2]\n",
    "        save_file.writelines(line[0]+' '+line[1]+','+str(line[2])[0:7]+','+str(line[3])+','+str(diff))\n",
    "        save_file.write('\\n')\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the number of senses for each wordpair (for Simverb)\n",
    "\n",
    "# Load dataset\n",
    "dataset = import_dataset('EN-SimVerb-3200-mod.txt')\n",
    "\n",
    "# Load number of senses for each word in SimVerb corpus\n",
    "with open(path_base+'Corpus Data//Dictionary Verb Corpus//Vocab lists//verb_polysemy_scores.txt', encoding='utf-8') as file:\n",
    "    lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "# dictionary with number of senses for each word\n",
    "polysemy_dict = {}\n",
    "for line in lines:\n",
    "    word_list = line.split()\n",
    "    word = word_list[0]\n",
    "    polysemy_score = word_list[1]\n",
    "    polysemy_dict[word] = int(polysemy_score)\n",
    "\n",
    "# dictionary with sum of word senses for each word pair\n",
    "full_polysemy_score_list = []\n",
    "for word_pair in dataset[0]:\n",
    "    total_polysemy_score = polysemy_dict[word_pair[0]] + polysemy_dict[word_pair[1]]\n",
    "    full_polysemy_score_list.append(total_polysemy_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e86f3c5951539f2722badccf95389aafd2846e6c6d91d84b623110a1b2c6697"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
